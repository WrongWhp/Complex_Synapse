\documentclass[9pt,twocolumn,twoside,lineno]{pnas-new}
% Use the lineno option to display guide line numbers if required.
% Note that the use of elements such as single-column equations
% may affect the guide line number alignment.

\templatetype{pnasresearcharticle} % Choose template
% {pnasresearcharticle} = Template for a two-column research article
% {pnasmathematics} = Template for a one-column mathematics article
% {pnasinvited} = Template for a PNAS invited submission

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% ADDITIONAL OPTIONAL STYLE FILES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
%\usepackage{graphics} for finding documentation only
\pdfsuppresswarningpagegroup=1
\usepackage{epstopdf}
\epstopdfsetup{update,suffix=-generated}
%
\IfFileExists{"C:/Progra\string~1/Inkscape/inkscape.exe"}{\newcommand{\inkscapedir}{C:/Progra\string~1/Inkscape/}}{}
\IfFileExists{"/Applications/Inkscape.app/Contents/Resources/bin/inkscape"}{\newcommand{\inkscapedir}{/Applications/Inkscape.app/Contents/Resources/bin/}}{}
\providecommand{\inkscapedir}{}
\epstopdfDeclareGraphicsRule{.svg}{pdf}{.pdf}{%
  "\inkscapedir inkscape" -f #1 -C -A \OutputFile}
%
\usepackage{adjustbox}
\newcommand{\aligntop}[1]{\adjustbox{valign=t}{#1}}
%\usepackage[inline]{enumitem}
\newlist{myenuma}{enumerate*}{4}
\setlist[myenuma]{label=(\alph*),ref=\alph*}
\newcommand{\panel}[1]{\item\aligntop{#1}}
\graphicspath{{"Figs/"}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb,amsfonts}
\usepackage{amsthm}
%\usepackage[notref,notcite]{showkeys}
%\usepackage[centertags]{amsmath}
%\usepackage{bibunits}
%\usepackage{hyperref}
\hypersetup{bookmarksopenlevel=1,bookmarksnumbered}
\usepackage[capitalise,nameinlink]{cleveref}
%\Crefname{appendix}{App.}{Apps.}
\creflabelformat{equation}{[#2#1#3]}
\crefrangelabelformat{equation}{[#3#1#4--#5#2#6]}
%\crefformat{equation}{#2Eq.~[#1]#3}
%\crefrangeformat{equation}{Eqs.~[#3#1#4--#5#2#6]}
%\crefmultiformat{equation}{Eqs.~[#2#1#3}{,2#1#3]}{,#2#1#3}{,#2#1#3]}
%\crefrangemultiformat{equation}{Eqs.~[#3#1#4--#5#2#6}{,#3#1#4--#5#2#6]}{,#3#1#4--#5#2#6}{,#3#1#4--#5#2#6]}
\newcommand{\main}{the main paper}
\newcommand{\supp}{supporting information}
% ----------------------------------------------------------------
\newtheoremstyle{sldefinition}%
  {3pt}%space above
  {3pt}%space below
  {}%body font
  {}%indent amount
  {\bfseries}%theorem head font
  {}%theorem head punctuation
  {\newline}%space after head
  {\thmname{#1}\thmnumber{ #2}:{\bfseries\thmnote{ #3}}}%head spec
\newtheoremstyle{slplain}%
  {3pt}%space above
  {3pt}%space below
  {}%body font
  {}%indent amount
  {\bfseries}%theorem head font
  {}%theorem head punctuation
  {\newline}%space after head
  {\thmname{#1}\thmnumber{ #2}{\bfseries\thmnote{ (#3)}}: }%head spec
%
\theoremstyle{slplain}
\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
%
\theoremstyle{sldefinition}
\newtheorem{defn}{Definition}
%
\theoremstyle{remark}
\newtheorem*{notn}{Notation}
\newtheorem*{rem}{Remark}
% ----------------------------------------------------------------

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% OPTIONAL MACRO DEFINITIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\ra}{\rightarrow}
\newcommand{\lr}{\leftrightarrow}
\newcommand{\cdt}{\!\cdot\!}
\newcommand{\vp}{\vspace{0.5cm}}
\newcommand{\hp}{\hspace{0.5cm}}
%
%e.g., i.e. with normal spaces
\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\cf}{cf.\ }
\newcommand{\etc}{etc.\ }
\newcommand{\wrt}{wrt.\ }
\newcommand{\Eg}{E.g.\ }
\newcommand{\Ie}{I.e.\ }
\newcommand{\Cf}{Cf.\ }
\newcommand{\Etc}{Etc.\ }
%
\usepackage{delim}
% brackets etc.
\delimdef\prn#1{\dleft( #1 \dright)}
\delimdef\brc#1{\dleft\{ #1 \dright\}}
\delimdef\brk#1{\dleft[ #1 \dright]}
\delimdef\abs#1{\dleft\lvert #1 \dright\rvert}
\delimdef\nrm#1{\dleft\lVert #1 \dright\rVert}
\delimdef\av#1{\dleft\langle #1 \dright\rangle}
\delimdef\avc#1#2{\dleft\langle #1 \dmiddle\vert #2 \dright\rangle}
%\newenvironment{cases}{\dleft\{\begin{aligned}}{\end{aligned}\dright.}
%
% Sets
\delimdef\set#1#2{\dleft\{ #1 \dmiddle\vert #2 \dright\}}
\delimdef\cond#1#2{\!\dleft( #1 \dmiddle\vert #2 \dright)}
\delimdef\condb#1#2{\!\dleft[ #1 \dmiddle\vert #2 \dright]}
%
% Derivatives, etc. First argument is optional.
\newcommand{\diff}[3][]{\frac{\mathrm{d}^{#1} #2}{\mathrm{d}{#3}^{#1}}}
\newcommand{\pdiff}[3][]{\frac{\partial^{#1} #2}{\partial {#3}^{#1}}}
\newcommand{\pdiffc}[3][]{\left(\frac{\partial #2}{\partial {#3}}\right)_{\!\!#1}}
\newcommand{\intd}[2][]{\int #1\!\!\dr #2\,}
%
% Un-italicised letters
\newcommand{\dr}{\mathrm{d}}
\newcommand{\e}{\mathrm{e}}
\newcommand{\ir}{\mathrm{i}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\Det}{Det}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\bias}{Bias}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\sech}{sech}
\DeclareMathOperator{\diag}{diag}
%
% The default \Im and \Re look crap
\renewcommand{\Im}{\operatorname{\mathfrak{Im}}}
\renewcommand{\Re}{\operatorname{\mathfrak{Re}}}
%
% logic
\newcommand{\means}{\Longleftrightarrow}
\newcommand{\requires}{\Longleftarrow}
%
%additional symbols:
\newcommand{\half}{\frac{1}{2}}
\newcommand{\CO}{\mathcal{O}}
\newcommand{\CL}{\mathcal{L}}
\newcommand{\dt}{\dr t}
%
\DeclareMathOperator{\SNR}{SNR}
\DeclareMathOperator{\snr}{SNR}
\newcommand{\wv}{\vec{w}}
\newcommand{\wvi}{\vec{w}_\text{ideal}}
%matrices
\newcommand{\inv}{^{-1}}
\newcommand{\dg}{^\mathrm{dg}}
\newcommand{\trans}{^\mathrm{T}}
\newcommand{\I}{\mathbf{I}}
%vec of ones
\newcommand{\onev}{\mathbf{e}}
%mat of ones
\newcommand{\onem}{\mathbf{E}}
%Markov matrix
\newcommand{\MM}{\mathbf{Q}}
%prob distributions
\newcommand{\prob}{\mathbf{p}}
\newcommand{\eq}{\boldsymbol{\pi}}
%first passage times
\newcommand{\fpt}{\mathbf{T}}
%off-diag first passage times
\newcommand{\fptb}{\overline{\fpt}}
%fundamental matrix
\newcommand{\fund}{\mathbf{Z}}
%other symbols for matrices
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\arow}{\boldsymbol{\xi}}
\newcommand{\Lb}{\boldsymbol{\Lambda}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\enc}{\mathbf{q}}
\newcommand{\frg}{\W^{\mathrm{F}}}
\newcommand{\F}{\boldsymbol{\Phi}}
%superscripts
\newcommand{\pot}{^{\text{pot}}}
\newcommand{\dep}{^{\text{dep}}}
\newcommand{\potdep}{^{\text{pot/dep}}}
%sets
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CA}{\mathcal{A}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\comp}{^\mathrm{c}}
%eigenmodes
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\CI}{\mathcal{I}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Optimal synaptic strategies for different timescales of memory}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use letters for affiliations, numbers to show equal authorship (if applicable) and to indicate the corresponding author
\author[a,1]{Subhaneil Lahiri}
\author[a]{Surya Ganguli}

\affil[a]{Department of Applied Physics, Stanford University, Stanford CA}

% Please give the surname of the lead author for the running footer
\leadauthor{Lahiri}

% Please add here a significance statement to explain the relevance of your work
\significancestatement{Authors must submit a 120-word maximum statement about the significance of their research paper written at a level understandable to an undergraduate educated scientist outside their field of speciality.}

% Please include corresponding author, author contribution and author declaration information
\authorcontributions{Please provide details of author contributions here.}
\authordeclaration{Please declare any conflict of interest here.}
%\equalauthors{\textsuperscript{1}A.O.(Author One) and A.T. (Author Two) contributed equally to this work (remove if not applicable).}
\correspondingauthor{\textsuperscript{1}To whom correspondence should be addressed. E-mail: sulahiri@stanford.edu}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Keywords are not mandatory, but authors are strongly encouraged to provide them. If provided, please include two to five keywords, separated by the pipe symbol, e.g:
\keywords{synaptic plasticity | learning | memory | neuroscience}

%\abbreviations{SNR, signal-to-noise ratio; LTP, long term potentiation;LTD, long term depression}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{article}
\begin{abstract}
  An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses.
  To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states.
  Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited memory capacity.
  This raises the fundamental question, how does synaptic complexity give rise to memory?
  To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks.
  Moreover, in proving such theorems, we uncover a framework, based on first passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function.

  Overall, we uncover general design principles governing the functional organization of complex molecular networks, and suggest new experimental observables in synaptic physiology, based on first %passage time theory, that connect molecular complexity to memory.
\end{abstract}

\dates{This manuscript was compiled on \today}
\doi{\url{www.pnas.org/cgi/doi/10.1073/pnas.XXXXXXXXXX}}

\setboolean{displaywatermark}{false}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{article}

% Optional adjustment to line up main text (after abstract) of first page with line numbers, when using both lineno and twocolumn options.
% You should only change this length when you've finalised the article contents.
\verticaladjustment{-2pt}

\maketitle
\thispagestyle{firststyle}
\ifthenelse{\boolean{shortarticle}}{\ifthenelse{\boolean{singlecolumn}}{\abscontentformatted}{\abscontent}}{}

\dropcap{I}t is widely thought that our very ability to remember the past over long time scales depends crucially on our ability to modify synapses in our brain in an experience dependent manner.
Classical models of synaptic plasticity model synaptic efficacy as an analog scalar value, denoting the size of a post-synaptic potential injected into one neuron from another.
Theoretical work has shown that such models have a reasonable, extensive memory capacity, in which the number of long term associations that can be stored by a neuron is proportional its number of afferent synapses \cite{Hopfield1982model,Amit1985hopfield,Gardner1988perceptron}.
However, recent experimental work has shown that many synapses are more digital than analog; they cannot robustly assume an infinite continuum of analog values, but rather can only take on a finite number of distinguishable strengths, a number than can be as small as two \cite{Bliss1993LTP,Petersen1998allornone,O'Connor2005switch} \cite[though see][] {Enoki2009graded}.
This one simple modification leads to a catastrophe in memory capacity:  classical models with digital synapses, when operating in a palimpsest mode in which the ongoing storage of new memories can overwrite previous memories, have a memory capacity proportional to the logarithm of the number of synapses \cite{amit1992constraints,amit1994learning}.
Intuitively, when synapses are digital, the storage of a new memory can flip a population of synaptic switches, thereby rapidly erasing previous memories stored in the same synaptic population. This result indicates that the dominant theoretical basis for the storage of long term memories in modifiable synaptic switches is flawed.

Recent work \cite{Fusi2005cascade,Fusi2007multistate,Leibold2008serial} has suggested that a way out of this logarithmic catastrophe is to expand our theoretical conception of a synapse from a single scalar value to an entire stochastic dynamical system in its own right.
This conceptual expansion is further necessitated by the experimental reality that synapses contain within them immensely complex molecular signaling pathways, with many internal molecular functional states \cite[\eg see][]{Bliss1993LTP,Bredt2003AMPA,Coba2009phosphorylation}.
While externally, synaptic efficacy could be digital, candidate patterns of electrical activity leading to potentiation or depression could yield transitions between these internal molecular states without necessarily inducing an associated change in synaptic efficacy.
This form of synaptic change, known as metaplasticity \cite{Abraham1996metaplasticity,Montgomery2002765}, can allow the probability of synaptic potentiation or depression to acquire a rich dependence on the history of prior changes in efficacy, thereby potentially improving memory capacity.

Theoretical studies of complex, metaplastic synapses have focused on analyzing the memory performance of a limited number of very specific molecular dynamical systems, characterized by a number of internal states in which potentiation and depression each induce a specific set of allowable transitions between states (\eg see \cref{fig:models} below).
While these models can vastly outperform simple binary synaptic switches, these analyses leave open several deep and important questions.
For example, how does the structure of a synaptic dynamical system determine its memory performance?  What are the fundamental limits of memory performance over the space of all possible synaptic dynamical systems?
What is the structural organization of synaptic dynamical systems that achieve these limits?
Moreover, from an experimental perspective, it is unlikely that all synapses can be described by a single canonical synaptic model;  just like the case of neurons, there is an incredible diversity of molecular networks underlying synapses both across species and across brain regions within a single organism \cite{Emes2012synapserev}.
In order to elucidate the functional contribution of this diverse molecular complexity to learning and memory, it is essential to move beyond the analysis of specific models and instead develop a general theory of learning and memory for complex synapses.
Moreover, such a general theory of complex synapses could aid in development of novel artificial memory storage devices.  % Sutskever, memory for learning?

Here we initiate such a general theory by proving upper bounds on the memory curve associated with any synaptic dynamical system, within the well established ideal
observer framework of \cite{Fusi2005cascade,Fusi2007multistate,Barrett2008discrete}.
Along the way we develop principles based on first passage time theory to order the structure of synaptic dynamical systems and relate this structure to memory performance.
We summarize our main results in the \hyperref[sec:disc]{discussion section}.








\section{Overall framework: synaptic models and their memory curves}\label{sec:setup}

In this section, we describe the class of models of synaptic plasticity that we are studying and how we quantify their memory performance.
In the subsequent sections, we will find upper bounds on this performance.

We use a well established formalism for the study of learning and memory with complex synapses (see \cite{Fusi2005cascade,Fusi2007multistate,Barrett2008discrete}).
In this approach, electrical patterns of activity corresponding to candidate potentiating and depressing plasticity events occur randomly and independently at all synapses at a Poisson rate $r$.  These events reflect possible synaptic changes due to either spontaneous network activity, or the storage of new memories.
We let  $f\pot$ and  $f\dep$ denote the fraction of these events that are candidate potentiating or depressing events respectively.
Furthermore, we assume our synaptic model has $M$ internal molecular functional states, and that a candidate potentiating (depotentiating) event induces a stochastic transition in the internal state described by an $M \times M$ discrete time Markov transition matrix  $\M\pot$ ($\M\dep$).
In this framework, the states of different synapses will be independent, and the entire synaptic population can be fully described by the probability distribution across these states, which we will indicate with the row-vector $\prob(t)$.
Thus the $i$'th component of $\prob(t)$ denotes the fraction of the synaptic population in state $i$.
Furthermore, each state $i$ has its own synaptic weight, $\w_i$, which we take, in the worst case scenario, to be restricted to two values.
After shifting and scaling these two values,  we can assume they are $\pm1$, without loss of generality.

We also employ an ``ideal observer'' approach to the memory readout, where the synaptic weights are read directly.
This provides an upper bound on the quality of any readout using neural activity.

For any single memory, stored at time $t=0$, we assume there will be an ideal pattern of synaptic weights across a population of $N$ synapses, the $N$-element vector $\wvi$, that is $+1$ at all synapses that experience a candidate potentiation event, and $-1$ at all synapses that experience a candidate depression event at the time of memory storage.
We assume that any pattern of synaptic weights close to $\wvi$ is sufficient to recall the memory.
However, the actual pattern of synaptic weights at some later time, $t$, will change to $\wv(t)$ due to further modifications from the storage of subsequent memories.
We can use the overlap between these, $\wvi\cdt\wv(t)$, as a measure of the quality of the memory.
As $t\to\infty$, the system will return to its steady state distribution which will be uncorrelated with the memory stored at $t=0$.
The probability distribution of the quantity $\wvi\cdt\wv(\infty)$ can be used as a ``null model'' for comparison.

The extent to which the memory has been stored is described by a signal-to-noise ratio (SNR) \cite{Fusi2005cascade,Fusi2007multistate}:
%
\begin{equation}\label{eq:SNRdef}
  \snr(t) = \frac{\av{\wv_\text{ideal}\cdt\wv(t)} - \av{\wv_\text{ideal}\cdt\wv(\infty)}}
     {\sqrt{\var(\wv_\text{ideal}\cdt\wv(\infty))}}.
\end{equation}
%
The noise in the denominator is essentially $\sqrt{N}$.
There is a correction when potentiation and depression are imbalanced, but this will not affect the upper bounds that we will discuss below and will be ignored in the subsequent formulae.

A simple average memory curve can be derived as follows.
All of the preceding plasticity events, prior to $t=0$, will put the population of synapses in its steady-state distribution, $\eq$.
The memory we are tracking at $t=0$ will change the internal state distribution to $\eq\M\pot$ (or $\eq\M\dep$) in those synapses that experience a candidate potentiation (or depression) event.
As the potentiating/depressing nature of the subsequent memories is independent of $\wvi$, we can average over all sequences, resulting in the evolution of the probability distribution:
%
\begin{multline}\label{eq:evol}
  \diff{\prob(t)}{t} = r\prob(t)\frg,
  \\ \text{where} \quad
  \frg = f\pot\M\pot + f\dep\M\dep - \I.
\end{multline}
%
Here $\frg$ is a continuous time transition matrix that models the process of forgetting the memory stored at time $t=0$ due to random candidate potentiation/depression events occurring at each synapse due to the storage of subsequent memories.
Its stationary distribution is $\eq$.


This results in the following SNR
%
\begin{equation}\label{eq:SNRcalc}
  \snr(t) = \sqrt{N}(2 f\pot f\dep) \eq \brk{\M\pot-\M\dep} \e^{rt\frg} \w.
\end{equation}
%
%The factor of $\eq$ describes the synapses being in the steady-state distribution before the memory is encoded. The factor of $(\M\pot-\M\dep)$ comes from the encoding of the memory at $t=0$, with $\wvi$ being $\pm1$ in synapses that are potentiated/depotentiated. The factor of $\e^{rt\frg}$ describes the subsequent evolution of the probability distribution, averaged over all sequences of plasticity events, and the factor of $\w$ indicates the readout via the synaptic weight.
A detailed derivation of this formula can be found in the \supp.
We will frequently refer to this function as the memory curve.
It can be thought of as the excess fraction of synapses (relative to equilibrium) that maintain their ideal synaptic strength at time $t$, as dictated by the stored memory at time $t=0$.


Much of the previous work on these types of complex synaptic models has focused on understanding the memory curves of specific models, or choices of $\M\potdep$.
Two examples of these models are shown in \cref{fig:models}. We see that they have different memory properties.
The serial model performs relatively well at one particular timescale, but it performs poorly at other times.
The cascade model does not perform quite as well at that time, but it maintains its performance over a wider range of timescales.

\begin{figure}[tbp]
 \begin{center}
 \begin{myenuma}
  \panel{\includegraphics[width=0.25\linewidth]{cascade.svg}}\label{fig:cascade}
  \panel{\includegraphics[width=0.25\linewidth]{multistate.svg}}\label{fig:multistate}
  \panel{\includegraphics[width=0.3\linewidth]{cascms.eps}}\label{fig:cascms}
 \end{myenuma}
 \end{center}
  \caption[Models of complex synapses]{Models of complex synapses.
  (\ref{fig:cascade}) The cascade model of \cite{Fusi2005cascade}, showing transitions between states of high/low synaptic weight (red/blue circles) due to potentiation/depression (solid red/dashed blue arrows).
  (\ref{fig:multistate}) The serial model of \cite{Leibold2008serial}.
  (\ref{fig:cascms}) The memory curves of these two models, showing the decay of the signal-to-noise ratio (to be defined \hyperref[sec:setup]{below}) as subsequent memories are stored.
  \label{fig:models}}
\end{figure}


In this work, rather than analyzing specific models,  we take a different approach, in order to obtain a more general theory.
We consider the \emph{entire} space of these models and find upper bounds on the memory capacity of \emph{any} of them.
The space of models with a fixed number of internal states $M$ is parameterized by the pair of $M \times M$ discrete time stochastic
transition matrices $\M\pot$ and $\M\dep$, in addition to $f\potdep$.  The parameters must satisfy the following constraints:
%
\begin{equation}\label{eq:constr}
\begin{gathered}
\begin{aligned}
  \M\potdep_{ij} &\in [0,1], &\qquad
  \sum_j \M\potdep_{ij} &= 1, \\
  f\potdep &\in [0,1],&
  f\pot + f\dep &= 1,
\end{aligned}
\\
  \eq\frg = 0, \qquad
  \sum_i \eq_i = 1, \qquad
  \w_i = \pm 1.
\end{gathered}
\end{equation}
%
The upper bounds on $\M\potdep_{ij}$ and $f\potdep$ follow automatically from the other constraints.

The critical question is: what do these constraints imply about the space of achievable memory curves in \cref{eq:SNRcalc}?
To answer this question, especially for limits on achievable memory at finite times, it will be useful to employ the eigenmode decomposition:
%
\begin{equation}\label{eq:eigendecomp}
\begin{aligned}
  \frg &= \sum_a -q_a \uv^a \vv^a,
  &\quad
  \vv^a \uv^b &= \delta_{ab},
  \\%\quad
  \frg \uv^a &= -q_a \uv^a,
  &\quad
  \vv^a \frg &= -q_a \vv^a.
\end{aligned}
\end{equation}
%
Here $q_a$ are the negative of the eigenvalues of the forgetting process $\frg$, $\uv^a$ are the right (column) eigenvectors and $\vv^a$ are the left (row) eigenvectors.
This decomposition allows us to write the memory curve as a sum of exponentials,
%
\begin{equation}\label{eq:SNReigen}
\begin{aligned}
  \snr(t) &= \sqrt{N}\sum_a \CI_a \e^{-rt/\tau_a},
%  &\quad \text{where}&\;&
%  \CI_a &= \prn{2 f\pot f\dep} \eq (\M\pot-\M\dep) \uv^a  \vv^a \w,\\&
%  & \text{and}&&
%  \tau_a &= \frac{1}{q_a}.
\end{aligned}
\end{equation}
%
where $\;\CI_a = (2 f\pot f\dep) \eq (\M\pot-\M\dep) \uv^a  \vv^a \w\;$ and $\;\tau_a = {1}/{q_a}$.
We can then ask the question: what are the constraints on these quantities, namely eigenmode initial SNR's, $\CI_a$, and time constants, $\tau_a$, implied by the constraints in \cref{eq:constr}?
We will derive some of these constraints in the next section.

\section{Upper bounds on achievable memory capacity}\label{sec:bounds}

In the previous section, in \cref{eq:SNRcalc} we have described an analytic expression for a memory curve as a function of the structure of a synaptic dynamical system, described by the pair of stochastic transition matrices  $\M\potdep$.
Since the performance measure for memory is an entire memory curve, and not just a single number, there is no universal scalar notion of optimal memory in the space of synaptic dynamical systems.
Instead there are tradeoffs between storing proximal and distal memories;  often attempts to increase memory at late (early) times by changing $\M\potdep$, incurs a performance loss in memory at early (late) times in specific models considered so far \cite{Fusi2005cascade,Fusi2007multistate,Leibold2008serial}.
Thus our end goal, achieved \hyperref[sec:env]{below}, is to derive an envelope memory curve in the SNR-time plane, or a curve that forms an upper-bound on the {\it entire} memory curve of {\it any} model.
In order to achieve this goal, in this section, we must first derive upper bounds, over the space of all possible synaptic models, on two different scalar functions of the memory curve: its initial SNR, and the area under the memory curve.
In the process of upper-bounding the area, we will develop an essential framework to organize the structure of synaptic dynamical systems based on first passage time theory.


\subsection{Bounding initial SNR}\label{sec:initial}

We now give an upper bound on the initial SNR,
%
\begin{equation}\label{eq:init}
  \snr(0) = \sqrt{N}\prn{2 f\pot f\dep} \eq \prn{\M\pot-\M\dep} \w,
\end{equation}
%
over \emph{all} possible models and also find the class of models that saturate this bound.
A useful quantity is the equilibrium probability flux between two disjoint sets of states, $\CA$ and $\CB$:
%
\begin{equation}\label{eq:flux}
  \F_{\CA\CB} = \sum_{i\in\CA} \sum_{j\in\CB} r \eq_i \frg_{ij}.
\end{equation}
%
The initial SNR is closely related to the flux from the states with $\w_i=-1$ to those with $\w_j=+1$ (see \supp):
%
\begin{equation}\label{eq:initflux}
  \snr(0) \leq \frac{4\sqrt{N}\F_{-+}}{r}\,.
\end{equation}
%
This inequality becomes an equality if potentiation never decreases the synaptic weight and depression never increases it, which should be a property of any sensible model.

To maximize this flux, potentiation from a weak state must be guaranteed to end in a strong state, and depression must do the reverse.
An example of such a model is shown in \cref{fig:max_init}(\ref{fig:max_init_pot},\ref{fig:max_init_dep}).
These models have a property known as ``lumpability'' (see \cite[\S6.3]{kemeny1960finite} for the discrete time version and \cite{burke1958markovian,Ball1993Lumpability} for continuous time).
They are completely equivalent to a two state model with transition probabilities equal to 1, as shown in \cref{fig:max_init}(\ref{fig:binary_det}).
In particular, they have the same memory curve.

This two state model has the equilibrium distribution $\eq=(f\dep,f\pot)$ and its flux is given by $\F_{-+} = rf\pot f\dep$.
This is maximized when $f\pot=f\dep=\half$, leading to the upper bound:
%
\begin{equation}\label{eq:max_init}
  \snr(0) \leq \sqrt{N}.
\end{equation}
%
We note that while this model has high initial SNR, it also has very fast memory decay -- with a timescale $\tau\sim\frac{1}{r}$.
As the synapse is very plastic, the initial memory is encoded very easily, but the subsequent memories also overwrite it rapidly.
This is one example of the tradeoff between optimizing memory at early versus late times.
\begin{figure}[tbp]
 \begin{center}
 \begin{myenuma}
  \panel{\hp\includegraphics[width=0.24\linewidth]{max_init_pot.svg}}\label{fig:max_init_pot}\hp
  \panel{\includegraphics[width=0.24\linewidth]{max_init_dep.svg}}\label{fig:max_init_dep}\hp
  \panel{\hp\includegraphics[width=0.1\linewidth]{binary_det.svg}}\label{fig:binary_det}
  \end{myenuma}
 \end{center}
  \caption[Synaptic models that maximize initial SNR]{Synaptic models that maximize initial SNR.
  (\ref{fig:max_init_pot}) For potentiation, all transitions starting from a weak state lead to a strong state, and the probabilities for all transitions leaving a given weak state sum to 1.
  (\ref{fig:max_init_pot}) Depression is similar to potentiation, but with strong and weak interchanged.
  (\ref{fig:binary_det}) The equivalent two state model, with transition probabilities under potentiation and depression equal to one.
  \label{fig:max_init}}
\end{figure}


\subsection{Imposing order on internal states through first passage times}\label{sec:order}


Our goal is to understand the relationship between structure and function in the space of all possible synaptic models.
This goal is complicated by the fact that this space contains many different possible network topologies, encoded in the nonzero matrix elements of $\M\potdep$.
To systematically analyze this entire space, we develop an important organizing principle using the theory of first passage times in the stochastic process of forgetting, described by $\frg$.
The mean first passage time matrix, $\smash{\fptb_{ij}}$, is defined as the average time it takes to reach state $j$ for the first time, starting from state $i$.
The diagonal elements are defined to be zero.

A remarkable theorem we will exploit is that the quantity
%
\begin{equation}\label{eq:kemenyconst}
  \eta \equiv \sum_j \fptb_{ij}\eq_j,
\end{equation}
%
is independent of the starting state $i$.
It is known as Kemeny's constant \cite[see][\S4.4]{kemeny1960finite}.
Intuitively, \cref{eq:kemenyconst} states that the average time it takes to reach any state, weighted by its equilibrium probability, is independent of the starting state, implying a hidden constancy inherent in any stochastic process.

In the context of complex synapses, we can define the partial sums
%
\begin{equation}\label{eq:kemenypm}
   \eta^+_i = \sum_{j\in+} \fptb_{ij} \eq_j,
   \qquad
   \eta^-_i = \sum_{j\in-} \fptb_{ij} \eq_j.
\end{equation}
%
These can be thought of as the average time it takes to reach the strong/weak states respectively.
Using these definitions, we can then impose an order on the states by arranging them in order of decreasing $\smash{\eta^+_i}$ or increasing $\smash{\eta^-_i}$.
Because $\smash{\eta^+_i+\eta^-_i=\eta}$ is independent of $i$, the two orderings are the same.
In this order, which depends sensitively on the structure of $\M\potdep$, states later (to the right in figures below) can be considered to be more potentiated than states earlier (to the left in figures below), despite the fact that they have the same synaptic efficacy.
In essence, in this order, a state is considered to be more potentiated if the average time it takes to reach all the strong efficacy states is shorter.
We will see that synaptic models that optimize various measures of memory have an exceedingly simple structure when, and only when, their states are arranged in this order.%
\footnote{Note that we do not need to worry about the order of the $\eta^\pm_i$ changing during the optimization: necessary conditions for a maximum only require that there is no infinitesimal perturbation that increases the area. Therefore we need only consider an infinitesimal neighborhood of the model, in which the order will not change.\label{ft:etaorder}}



\subsection{Bounding area}\label{sec:area}

Now consider the area under the memory curve:
%
\begin{equation}\label{eq:area}
  A = \int_0^\infty\!\!\dr t \, \snr(t).
\end{equation}
%
We will find an upper bound on this quantity as well as the model that saturates this bound.


First passage time theory introduced in the \hyperref[sec:order]{previous section} becomes useful because the area has a simple expression in terms of quantities introduced in \cref{eq:kemenypm} (see \supp):
%
\begin{equation}\label{eq:areaEta}
\begin{aligned}
  A &= \sqrt{N}(4f\pot f\dep) \sum_{ij} \eq_i \!\brk{\M\pot_{ij}-\M\dep_{ij}}\! (\eta^+_i-\eta^+_j)\\
    &= \sqrt{N}(4f\pot f\dep) \sum_{ij} \eq_i \!\brk{\M\pot_{ij}-\M\dep_{ij}}\! (\eta^-_j-\eta^-_i).
\end{aligned}
\end{equation}
%


With the states in the order described above, we can find perturbations of $\M\potdep$ that will always increase the area, whilst leaving the equilibrium distribution, $\eq$, unchanged.
Some of these perturbations are shown in \cref{fig:perts}, see \supp\ for details.
For example, in \cref{fig:perts}(\ref{fig:tri_right}), for two states $i$ on the left and $j$ on the right, with $j$ being more ``potentiated" than $i$ (i.e. $\smash{\eta^+_i > \eta^+_j}$), we have proven that increasing $\smash{\M\pot_{ij}}$ and decreasing $\smash{\M\dep_{ij}}$ leads to an increase in area.
The only thing that can prevent these perturbations from increasing the area is when they require the decrease of a matrix element that has already been set to 0.
This determines the topology (non-zero transition probabilities) of the model with maximal area.
It is of the form shown in \cref{fig:env}(\ref{fig:sticky}),%\cref{fig:max_area},
with potentiation moving one step to the right and depression moving one step to the left.
Any other topology would allow some class of perturbations (\eg in \cref{fig:perts}) to further increase the area.


\begin{figure}[tbp]
 \begin{center}
 \begin{myenuma}
  \panel{\includegraphics[width=0.14\linewidth]{triangular_right.svg}}\label{fig:tri_right}\hp
  \panel{\includegraphics[width=0.14\linewidth]{triangular_left.svg}}\label{fig:tri_left}\hp
  \panel{\includegraphics[width=0.28\linewidth]{shortcut.svg}}\label{fig:shortcut}
 \end{myenuma}
 \end{center}
  \caption[Perturbations that increase the area]{Perturbations that increase the area.
  (\ref{fig:tri_right}) Perturbations that increase elements of $\M\pot$ above the diagonal and decrease the corresponding elements of $\M\dep$. It can no longer be used when $\M\dep$ is lower triangular, \ie depression must move synapses to ``more depressed'' states.
  (\ref{fig:tri_left}) Perturbations that decrease elements of $\M\pot$ below the diagonal and increase the corresponding elements of $\M\dep$. It can no longer be used when $\M\pot$ is upper triangular, \ie potentiation must move synapses to ``more potentiated'' states.
  (\ref{fig:shortcut}) Perturbation that decreases ``shortcut'' transitions and increases the bypassed ``direct'' transitions. It can no longer be used when there are only nearest-neighbor ``direct'' transitions.
  \label{fig:perts}}
\end{figure}

As these perturbations do not change the equilibrium distribution, this means that the area of \emph{any} model is bounded by that of a linear chain with the same equilibrium distribution.
The area of a linear chain model can be expressed directly in terms of its equilibrium state distribution, $\eq$, yielding the following upper bound on the area of any model with the same $\eq$ (see \supp):
%
\begin{equation}\label{eq:multistatearea}
\begin{aligned}
  A &\leq \frac{2\sqrt{N}}{r} \sum_k \brk{k - \sum_j j\eq_j} \eq_k \w_k \\
    &= \frac{2\sqrt{N}}{r} \sum_k \abs{k - \sum_j j\eq_j} \eq_k ,
\end{aligned}
\end{equation}
%
where we chose $\w_k=\sgn[{k - \sum_j j\eq_j}]$.
We can then maximize this by pushing all of the equilibrium distribution symmetrically to the two end states.
This can be done by reducing the transition probabilities out of these states, as in \cref{fig:env}(\ref{fig:sticky}). %\cref{fig:max_area}.
This makes it very difficult to exit these states once they have been entered.
The resulting area is
%
\begin{equation}\label{eq:max_area}
  A \leq \frac{\sqrt{N}(M-1)}{r}.
\end{equation}
%
This analytical result is similar to a numerical result found in \cite{Barrett2008discrete} under a slightly different information theoretic measure of memory performance.

The ``sticky'' end states result in very slow decay of memory, but they also make it difficult to encode the memory in the first place, since a small fraction of synapses are able to change synaptic efficacy during the storage of a new memory.
Thus models that maximize area optimize memory at late times, at the expense of early times.

%\begin{figure}[tbp]
% \begin{center}
%  \includegraphics[width=0.35\linewidth]{multistate_sticky.svg}
% \end{center}
%  \caption{Model that maximizes the area.
%  Unlabeled transitions have probability 1.
%  Labeled transitions have probability $\varepsilon\to0$.
%  }\label{fig:max_area}
%\end{figure}



\section{Memory curve envelope}\label{sec:env}

Now we will look at the implications of the upper bounds found in the \hyperref[sec:bounds]{previous section} for the SNR at finite times.
As argued in \cref{eq:SNReigen}, the memory curve can be written in the form
%
\begin{equation}\label{eq:SNRmodes}
  \snr(t) = \sqrt{N}\sum_a \CI_a \e^{-rt/\tau_a}.
\end{equation}
%
The upper bounds on the initial SNR, \cref{eq:max_init}, and the area, \cref{eq:max_area}, imply the following constraints on the parameters $\brc{\CI_a,\tau_a}$:
%
\begin{equation}\label{eq:modeconstr}
  \sum_a \CI_a \leq 1,
  \qquad\qquad
  \sum_a \CI_a \tau_a \leq M-1.
\end{equation}
%
We are not claiming that these are a complete set of constraints: not every set $\brc{\CI_a,\tau_a}$ that satisfies these inequalities will actually be achievable by a synaptic model.
However, any set that violates either inequality will definitely not be achievable.

Now we can pick some fixed time, $\smash{t_0}$, and maximize the SNR at that time \wrt the parameters $\smash{\brc{\CI_a,\tau_a}}$, subject to the constraints above.
This always results in a single nonzero $\smash{\CI_a}$; in essence, optimizing memory at a single time requires a single exponential.  The resulting optimal memory curve, along with the achieved memory at the chosen time, depends on $t_0$ as follows:
%
\begin{equation}\label{eq:env}
\begin{aligned}
  t_0 &\leq \frac{M-1}{r} &
   & \implies &
  \snr(t) &= \sqrt{N}\e^{-rt/(M-1)} \\&&
   & \implies &
  \snr(t_0) &= \sqrt{N}\e^{-rt_0/(M-1)},\\
  t_0 &\geq \frac{M-1}{r} &
   & \implies &
  \snr(t) &= \frac{\sqrt{N}(M-1)\e^{-t/t_0}}{rt_0} \\&&
   & \implies &
  \snr(t_0) &= \frac{\sqrt{N}(M-1)}{\e rt_0}.
\end{aligned}
\end{equation}
%
Both the initial SNR bound and the area bound are saturated at early times.
At late times, only the area bound is saturated.
The function $\snr(t_0)$, the green curve in \cref{fig:env}(\ref{fig:envplot}), above forms a memory curve envelope with late-time power-law decay $\sim \smash{t_0\inv}$.
No synaptic model can have an SNR that is greater than this at any time.
We can use this to find an upper bound on the memory lifetime, $\tau(\epsilon)$, by finding the point at which the envelope crosses $\epsilon$:
%
\begin{equation}\label{eq:lifebound}
  \tau(\epsilon) \leq \frac{\sqrt{N}(M-1)}{\epsilon\e r},
\end{equation}
%
where we assume $N>(\epsilon\e)^2$.
Intriguingly, both the lifetime and memory envelope expand linearly with the number of internal states $M$, and increase as the square root of the number of synapses $N$.

This leaves the question of whether this bound is achievable.
At any time, can we find a model whose memory curve touches the envelope?
The red curves in \cref{fig:env}(\ref{fig:envplot}) show the closest we have come to the envelope with actual models, by repeated numerical optimization of $\SNR(t_0)$ over $\M\potdep$ with random initialization and by hand designed models.

We see that at early, but not late times, there is a gap between the upper bound that we can prove and what we can achieve with actual models.
There may be other models we haven't found that could beat the ones we have, and come closer to our proven envelope.
However, we suspect that the area constraint is not the bottleneck for optimizing memory at times less than  $\CO(\frac{M}{r})$.
We believe there is some other constraint that prevents models from approaching the envelope, and currently are exploring several mathematical conjectures for the precise form of this constraint in order to obtain a potentially tighter envelope.
Nevertheless, we have proven rigorously that no model's memory curve can ever exceed this envelope, and that it is at least tight for late times, longer than $\CO(\frac{M}{r})$, where models of the form in \cref{fig:env}(\ref{fig:sticky})%\cref{fig:max_area}
 can come close to the envelope.

\begin{figure}
 \begin{center}
 \parbox[t]{0.43\linewidth}{%
 \begin{myenuma}
   \panel{\includegraphics[width=0.85\linewidth]{env.svg}}\label{fig:envplot}
 \end{myenuma}}\hp
 \parbox[t]{0.44\linewidth}{%
 \begin{myenuma}
  \stepcounter{myenumai}
% \addtocounter{enumi}{1}
   \panel{\includegraphics[width=0.8\linewidth]{diffjump.svg}}\label{fig:diffjump}\\[0.1\linewidth]
   \panel{\includegraphics[width=0.8\linewidth]{multistate_sticky.svg}}\label{fig:sticky}
 \end{myenuma}}
 \end{center}
  \caption[The memory curve envelope for $N=100$, $M=12$]{The memory curve envelope for $N=100$, $M=12$.
  (\protect\protect\ref{fig:envplot}) An upper bound on the SNR at any time is shown in green.
  The red dashed curve shows the result of numerical optimization of synaptic models with random initialization.
  The solid red curve shows the highest SNR we have found with hand designed models.
  At early times these models are of the form shown in (\ref{fig:diffjump}) with different numbers of states, and all transition probabilities equal to $1$.
  At late times they are of the form shown in (\ref{fig:sticky}) with different values of $\varepsilon$.
  The model shown in (\ref{fig:sticky}) also saturates the area bound \cref{eq:max_area} in the limit $\varepsilon\to0$.
  \label{fig:env}}
\end{figure}







\section{Discussion}\label{sec:disc}

We have initiated the development of  a general theory of learning and memory with complex synapses, allowing for an exploration of the entire space of complex synaptic models, rather than analyzing individual models one at a time.
In doing so, we have obtained several new mathematical results delineating the functional limits of memory achievable by synaptic complexity, and the structural characterization of synaptic dynamical systems that achieve these limits.
In particular, operating within the ideal observer framework of \cite{Fusi2005cascade,Fusi2007multistate,Barrett2008discrete}, we have shown that for a population of $N$ synapses with $M$ internal states, \begin{myenuma}
\item the initial SNR of any synaptic model cannot exceed $\sqrt{N}$, and any model that achieves this bound is equivalent to a binary synapse,
\item the area under the memory curve of any model cannot exceed that of a linear chain model with the same equilibrium distribution,
\item both the area and memory lifetime of any model cannot exceed $\CO(\sqrt{N}M)$, and the model that achieves this limit has a linear chain topology with only nearest neighbor transitions,
\item we have derived an envelope memory curve in the SNR-time plane that cannot be exceeded by the memory curve of any model, and models that approach this envelope for times greater than $\CO(\frac{M}{r})$ are linear chain models, and
\item this late-time envelope is a power-law proportional to $\smash{\CO({\sqrt{N} M}/{rt})}$, indicating that synaptic complexity can strongly enhance the limits of achievable memory.
\end{myenuma}

This theoretical study opens up several avenues for further inquiry.
In particular, the tightness of our envelope for early times, less than $\CO(\frac{M}{r})$, remains an open question, and we are currently pursuing several conjectures.
We have also derived memory constrained envelopes, by asking in the space of models that achieve a given SNR at a given time, what is the maximal SNR achievable at other times.
If these two times are beyond a threshold separation, optimal constrained models require two exponentials.
It would be interesting to systematically analyze the space of models that achieve good memory at multiple times, and understand their structural organization, and how they give rise to multiple exponentials, leading to power law memory decays.

%Another important consideration is the tolerance of the synapses to biological noise, which could be investigated by imposing lower limits on the level of stochasticity of the models.
Finally, it would be interesting to design physiological experiments in order to perform optimal systems identification of potential Markovian dynamical systems hiding within biological synapses,
given measurements of pre and post-synaptic spike trains along with changes in post-synaptic potentials.
Then given our theory, we could match this measured synaptic model to optimal models to understand for which timescales of memory, if any, biological synaptic dynamics may be tuned.

In summary, we hope that a deeper theoretical understanding of the functional role of synaptic complexity, initiated here, will help advance our understanding of the neurobiology of learning and memory, aid in the design of engineered memory circuits, and lead to new mathematical theorems about stochastic processes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\acknow{We thank Sloan, Genenetech, Burroughs-Wellcome, and Swartz foundations for support.
We thank Larry Abbott, Marcus Benna, Stefano Fusi, Jascha Sohl-Dickstein and David Sussillo for useful discussions.}
\showacknow % Display the acknowledgments section

% \pnasbreak splits and balances the columns before the references.
% If you see unexpected formatting errors, try commenting out this line
% as it can run into problems with floats and footnotes on the final page.
%\pnasbreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\bibliographystyle{pnas-new}
\bibliography{maths,neuro,markov}


%\end{article}
%\clearpage

\begin{strip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\appendix
\pdfbookmark[0]{Supporting information}{appendix}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\begin{center}
  \textbf{\Large Supporting information}
\end{center}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Here we provide more details underlying the derivations of results in \main.

\section{Continuous time Markov processes}\label{sec:ContMarkov}

In this section we'll provide a summary of all the relevant properties of ergodic Markov chains in continuous time to define notation.
It is a generalization of material that can be found in \cite{kemeny1960finite} with some ideas from \cite{hunter2000survey}.


\subsection{Notation}\label{sec:not}

For any matrix $\mathbf{A}$, we define matrices $\mathbf{A}\dg$ and $\overline{\mathbf{A}}$ as
%
\begin{equation}\label{eq:dgdef}
  \mathbf{A}\dg_{ij} \equiv \delta_{ij}\mathbf{A}_{ij},
  \qquad
  \overline{\mathbf{A}} \equiv \mathbf{A}-\mathbf{A}\dg.
\end{equation}
%
We let $\onev$ denote a column-vector of ones and $\onem=\onev\onev\trans$ denote a matrix of ones.


A continuous time Markov process is described by a matrix of transitions rates, $\MM_{ij}$, from state $i$ to $j$ with row sums equal to zero ($\MM\onev=0$).
The probabilities of being in each state at time $t$, the row-vector $\mathbf{p}(t)$, evolve according to
%
\begin{equation}\label{eq:rowsum}
  \diff{\mathbf{p}(t)}{t} = \mathbf{p}(t) \MM,
\end{equation}
%
where $\mathbf{p}(t)\onev=1$.

The equilibrium probabilities, $\eq$, satisfy
%
\begin{equation}\label{eq:equilibrium}
  \eq\MM=0,
  \qquad
  \eq\onev=1.
\end{equation}
%
As we assume an ergodic process, this eigenvalue is non-degenerate.
If all other eigenvalues have strictly negative real parts, the process is regular (aperiodic).

We define additional matrices
%
\begin{equation}\label{eq:defDLP}
  \Lb \equiv (-\MM\dg)\inv,
  \qquad
  \Pb \equiv \I + \Lb\MM.
\end{equation}
%
It can be shown that $\Lb_{ii}$ is the mean time it takes to leave state $i$ and $\Pb_{ij}$ is the probability the the next transition from state $i$ goes to state $j$:
%
\begin{equation}\label{eq:LamdaPcmpt}
  \Lb_{ii} = \frac{1}{\sum_{j \neq j} \MM_{ij}},
  \qquad
  \Pb_{ij} = %\left\{
%  \begin{aligned}
%     &0                                         && \text{if }i=j, \\
%     &\frac{\MM_{ij}}{\sum_{k \neq j} \MM_{ik}} &&\text{otherwise}.
%  \end{aligned}
%  \right.
  \begin{cases}
     0                                         &\text{if }i=j, \\
     \frac{\MM_{ij}}{\sum_{k \neq j} \MM_{ik}} &\text{otherwise}.
  \end{cases}
\end{equation}
%
Furthermore, we also define
%
\begin{equation}\label{eq:pdotD}
  \D \equiv \operatorname{diag}(\eq)\inv,
  \qquad\implies\qquad
  \eq\D=\onev\trans.
\end{equation}
%

\subsection{Fundamental matrix}\label{sec:fund}

For our results below regarding the integral of the memory curve, it can be useful to invert the stochastic transition matrix, $\MM$.
However, since $\MM$ has a zero eigenvalue, it cannot be inverted.
For this reason, the fundamental matrix arises as a useful surrogate for the inverse of $\MM$.
It is related to the first passage times, as we will see in the \hyperref[sec:fpt]{next subsection}.
Here we define the fundamental matrix and review its properties.

\begin{defn}[Fundamental matrix]
  For discrete time, the generalized fundamental matrix was defined in \cite{Kemeny1981fund}.
  For continuous time, we define:
  %
  \begin{equation}\label{eq:funddef}
    \fund \equiv (-\MM + \onev\arow)\inv,
  \end{equation}
  %
  where $\arow$ is any row-vector with $\arow\onev=1/\tau\neq0$.
\end{defn}
Note that the canonical choice for the discrete time version, $\arow=\eq$, is not available here due to problems with units.
It will be helpful to choose $\arow$ to be independent of $\MM$, \eg $\arow=\onev\trans/(n\tau)$.
All quantities that we calculate using $\fund$ below will be independent of this choice.

\begin{thm}
  The definition of $\fund$ is valid, \ie $(-\MM + \onev\arow)$ is invertible.
\end{thm}
\begin{proof}
  Assume there exists an $\mathbf{x}$ such that
  %
  \begin{equation}\label{eq:fundinvkern}
    (-\MM + \onev\arow)\mathbf{x}=0.
  \end{equation}
  %
  Multiplying from the left with $\eq$ gives
  %
  \begin{equation}\label{eq:fundinvpix}
    \arow \mathbf{x} = 0.
  \end{equation}
  %
  Substituting back into \cref{eq:fundinvkern} gives
  %
  \begin{equation*}
    \MM\mathbf{x}=0.
  \end{equation*}
  %
  As we assume an ergodic process, the zero eigenvalue is non-degenerate.
  Therefore, $\mathbf{x}=\lambda\onev$.
  Substituting this into \cref{eq:fundinvpix} gives
  %
  \begin{equation*}
    \lambda\arow\onev = \frac{\lambda}{\tau} = 0.
  \end{equation*}
  %
  As we defined $\arow$ such that $1/\tau\neq0$, this means $\lambda=0 \implies \mathbf{x}=0$.
\end{proof}

\begin{cor}
  %
  \begin{align}
    \arow\fund &= \eq, \label{eq:fundprob}\\
    \fund\onev &= \tau\onev,\label{eq:fundrowsum}\\
    \I+\MM\fund &= \onev\eq, \label{eq:fundQZ}\\
    \I+\fund\MM &= \tau\onev\arow. \label{eq:fundZQ}
  \end{align}
  %
\end{cor}
\begin{proof}
  We can deduce \cref{eq:fundprob,eq:fundrowsum} be pre/post-multiplying the following equations by $\fund$:
  %
  \begin{equation*}
    \begin{aligned}
      \eq(-\MM + \onev\arow) &= \arow, \\
      (-\MM + \onev\arow)\onev &= \frac{\onev}{\tau}.
    \end{aligned}
  \end{equation*}
  %
  We can then deduce \cref{eq:fundQZ,eq:fundZQ} by substituting these into
  %
  \begin{equation*}
    (-\MM + \onev\arow)\fund = \fund(-\MM + \onev\arow) = \I.
  \end{equation*}
  %
\end{proof}

%We will not need to use the concept of a g-inverse anywhere, so you can skip ahead to \cref{sec:fpt} now. For the sake of completeness, we can define $\MM^\#$ as
%%
%\begin{equation}\label{eq:ginvdef}
%  \MM^\# \equiv \tau\onev\eq - \fund
%\end{equation}
%%
%\begin{thm}
%  The matrix $\MM^\#$ satisfies the following defining properties of a g-inverse of $\MM$:
%  %
%  \begin{align}
%    \MM^\# \MM \MM^\# &= \MM^\#,     \label{eq:ginvpropo} \\
%    \MM \MM^\# \MM    &= \MM,        \label{eq:ginvpropi} \\
%    \MM \MM^\#        &= \MM^\# \MM, \label{eq:ginvcomm}
%  \end{align}
%  %
%  with \cref{eq:ginvcomm} holding iff $\eq$ and $\arow$ are proportional to each other.
%\end{thm}
%\begin{proof}
%  Using \cref{eq:rowsum}, \cref{eq:equilibrium}, \cref{eq:fundprob} and \cref{eq:fundrowsum}, one can verify
%  %
%  \begin{alignat*}{2}
%    \MM \MM^\# &= -\MM \fund,
%    &\qquad
%    \arow \MM^\# &= 0,
%    \\
%    \MM^\# \MM &= -\fund \MM,
%    &
%    \MM^\# \onev &= 0.
%  \end{alignat*}
%  %
%  Substituting into \cref{eq:fundQZ} and \cref{eq:fundZQ} leads to
%  %
%  \begin{equation*}
%    \I - \MM \MM^\# = \onev \eq,
%    \qquad
%    \I - \MM^\# \MM = \tau \onev \arow.
%  \end{equation*}
%  %
%  Pre/post-multiplying these by $\MM$ or $\MM^\#$ produces \cref{eq:ginvpropo} and \cref{eq:ginvpropi}. Subtracting produces
%  %
%  \begin{equation*}
%    [\MM,\MM^\#] = \onev(\tau\arow-\eq),
%  \end{equation*}
%  %
%  which vanishes iff $\eq$ and $\arow$ are proportional to each other (due to $\arow\onev=1/\tau$ and $\eq\onev=1$, the constant of proportionality could only be $\tau$).
%\end{proof}


\subsection{First passage times}\label{sec:fpt}

\begin{defn}[First passage time matrix]
 We define $\fptb_{ij}$ as the mean time it takes the process to reach state $j$ for the first time, starting from state $i$.
 We also define $\fpt\dg_{ii}$ as the mean time it takes the process to return to state $i$.
 As usual, $\fpt=\fptb+\fpt\dg$.
\end{defn}

This matrix is given by
%
\begin{equation}\label{eq:fpt}
  \fpt=(\onem\fund\dg - \fund + \Lb)\D,
\end{equation}
%
see \cite{Yao1985fpt} for a proof.
We can separate this into its diagonal and off-diagonal pieces.

The recurrence times are given by
%
\begin{equation}\label{eq:recurtime}
  \fpt\dg = \Lb\D.
\end{equation}
%
or in component form
%
\begin{equation*}
  \eq_i \Lb_{ii}\inv \fpt\dg_{ii} = 1.
\end{equation*}
%
The extra factor of $\Lb_{ii}$, compared to the discrete case \cite[Th.4.4.5]{kemeny1960finite}, occurs because in this case we are demanding that the process leaves the initial state once before returning, whereas in the discrete case we only measure the time it takes to go to the initial state after the first time-step.

The off-diagonal mean first passage times are given by
%
\begin{equation}\label{eq:fptfund}
  \fptb = (\onem\fund\dg - \fund)\D.
\end{equation}
%
or in component form:
%
\begin{equation}\label{eq:fptfundcmpt}
  \fptb_{ij} = \frac{\fund_{jj}-\fund_{ij}}{\eq_j}.
\end{equation}
%

\subsection{Mixing time (Kemeny's constant)}\label{sec:mixtime}

\begin{thm} \mbox{}\label{th:kemenyconst}
  The quantity
  %
  \begin{equation}\label{eq:mixdef}
    \eta \equiv \sum_j \fptb_{ij}\eq_j
  \end{equation}
  %
  is independent of $i$.
\end{thm}
\begin{proof}
  For discrete time, a proof can be found in \cite[Th.4.4.10]{kemeny1960finite}.
  For continuous time, we use \cref{eq:fptfund}, \cref{eq:fundrowsum} and the transpose of \cref{eq:pdotD}:
  %
  \begin{equation*}
    \begin{aligned}
      \fptb (\eq)\trans &= (\onem\fund\dg - \fund) \D (\eq)\trans \\
        &= (\onev\onev\trans\fund\dg - \fund) \onev \\
        &= (\onev\trans\fund\dg\onev)\onev - \fund\onev \\
        &= (\tr\fund - \tau)\onev.
    \end{aligned}
  \end{equation*}
  %
  which proves \cref{eq:mixdef} with $\eta = \tr\fund - \tau$.
\end{proof}

Note that it is essential that we use $\fptb$ and not $\fpt$ here, as that would lead to $\eta_i=\eta+\Lb_{ii}$, unlike the discrete time version, where this would only shift $\eta$ by 1.

\subsection{Sensitivity of equilibrium distribution}\label{sec:sensitivity}

Suppose that the Markov process, defined by $\MM$, depends on some parameter $\alpha$.
Differentiating \cref{eq:funddef} gives
%
\begin{equation}\label{eq:diffZ}
  \diff{\fund}{\alpha} = \fund \diff{\MM}{\alpha} \fund.
\end{equation}
%
We can substitute this into the derivative of \cref{eq:fundprob}:
%
\begin{equation}\label{eq:diffp}
  \diff{\eq}{\alpha} = \arow \fund \diff{\MM}{\alpha} \fund = \eq \diff{\MM}{\alpha} \fund.
\end{equation}
%
We can rewrite this in component form and use the fact that $\MM_{ii} = - \sum_{i\neq j} \MM_{ij}$:
%
\begin{equation}\label{eq:diffpT}
\begin{aligned}
  \diff{\eq_k}{\alpha} &= \sum_{i,j} \eq_i \diff{\MM_{ij}}{\alpha} \fund_{jk} \\
    &= \sum_{i\neq j} \eq_i \diff{\MM_{ij}}{\alpha} \fund_{jk} + \sum_i \eq_i \diff{\MM_{ii}}{\alpha} \fund_{ik} \\
    &= \sum_{i\neq j} \eq_i \diff{\MM_{ij}}{\alpha} (\fund_{jk} - \fund_{ik}) \\
    &= \sum_{i\neq j} \eq_i \diff{\MM_{ij}}{\alpha} \eq_k (\fptb_{ik} - \fptb_{jk}).
\end{aligned}
\end{equation}
%
This is a generalization of a result of \cite{cho2000markov} from discrete to continuous time that we will need below.
Note that the summand vanishes for $i=j$, so we can drop the restriction $i\neq j$ from the range of the sum.

\subsection{Subsets and flux}\label{sec:subsets}

Let us denote the set of states by $\CS$.
Consider a subset $\CA\subset\CS$.
We can define a projection operator onto this subset:
%
\begin{equation}\label{eq:proj}
  \prn{\I^\CA}_{ij} =
    \begin{cases}
      1 &\text{if $i=j\in\CA$,}\\
      0 &\text{otherwise.}
    \end{cases}
\end{equation}
%
We will use superscripts/subscripts to denote projection onto/summation over a subset:
%
\begin{equation}\label{eq:projsum}
  \begin{aligned}
    \arow^\CA &=\arow\I^\CA, &
    \M^{\cdot\CA}&=\M\I^\CA, &
    \M^{\CA\cdot}&=\I^\CA\M, &
    \mathbf{x}^\CA &= \I^\CA\mathbf{x},
    \\
    \arow_\CA &=\arow\onev^\CA, &
    \M_{\cdot\CA}&=\M\onev^\CA, &
    \M_{\CA\cdot}&=\prn{\onev^\CA}\trans\M, &
    \mathbf{x}_\CA &= \prn{\onev^\CA}\trans\mathbf{x},
  \end{aligned}
\end{equation}
%
where $\arow$ is a row vector, $\M$ is a matrix and $\mathbf{x}$ is a column vector.

We can define a flux matrix, a.k.a.\ ergodic flow:
%
\begin{equation}\label{eq:flux:s}
  \F = \D\inv\MM,
  \qquad
  \F_{ij} = \eq_i \MM_{ij}.
\end{equation}
%
This measures the flow of probability between states in the equilibrium distribution.
Detailed balance, a.k.a.\ reversibility, is equivalent to $\F=\F\trans$.

The flux between two subsets is a particularly useful quantity:
%
\begin{equation}\label{eq:subflux}
  \F_{\CA\CB} = {\eq}^\CA\MM\onev^\CB.
\end{equation}
%
One can show that
%
\begin{equation}\label{eq:compflux}
  \F_{\CA\CA\comp} = \F_{\CA\comp\CA} = -\F_{\CA\CA} = -\F_{\CA\comp\CA\comp}
\end{equation}
%
using $\prn{{\eq}^\CA + {\eq}^{\CA\comp}}\MM=0$ and $\MM\prn{{\onev}^\CA + {\onev}^{\CA\comp}}=0$.

\subsection{Lumpability}\label{sec:lump}

Suppose we have partitioned the states into disjoint subsets, $\brc{\CA_\alpha}$:
%
\begin{equation}\label{eq:partition}
  \bigcup_\alpha \CA_\alpha = \CS,
  \qquad
  \CA_\alpha \cap \CA_\beta = \delta_{\alpha\beta}\CA_\alpha.
\end{equation}
%
We will use $\alpha$ instead of $\CA_\alpha$ in superscripts and subscripts in the following.
The fact that these subsets are disjoint and exhaustive allows us to define the function
%
\begin{equation}\label{eq:whichsey}
  \sigma(i)=\alpha
  \qquad\means\qquad
  i\in\CA_\alpha.
\end{equation}
%

We can use this partition to define a new stochastic process associated with the original Markov chain.
At time $t$, if the state of the original process is $i$, the state of the new process is $\sigma(i)$.

One may ask if this new process is a Markov chain.
The answer is yes, if the original Markov chain has a property called lumpability \wrt the partition
(see \cite[\S6.3]{kemeny1960finite} for the discrete time version and \cite{burke1958markovian,Ball1993Lumpability} for continuous time):
%
\begin{equation}\label{eq:lump}
  \sigma(i)=\sigma(j)
  \quad\implies \quad
  \MM_{i\alpha}=\MM_{j\alpha} \equiv \sum_{k\in\CA_\alpha} \MM_{jk},
\end{equation}
%
\ie the total transition rate from some state to some subset is the same for all starting states within the same subset.
This common value is the transition rate for the new lumped Markov chain.

This can be rewritten with the aid of two matrices
%
\begin{equation}\label{eq:lumpmats}
  U_{\alpha i} = \frac{\delta_{\alpha\sigma(i)}}{\abs{\CA_\alpha}},
  \qquad
  V_{i\alpha} = \delta_{\sigma(i)\alpha}.
\end{equation}
%
Left multiplication by $U$ averages over subsets, right multiplication by $V$ sums over subsets.
For $U$, we chose the uniform measure in each subset. Any measure would work equally well, \eg one proportional to the equilibrium distribution:
%
\begin{equation}\label{eq:altlumpmats}
  U_{\alpha i} = \frac{{\eq_i}^\alpha}{\eq_\alpha}.
\end{equation}
%

One can show that $(UV)_{\alpha\beta}=\delta_{\alpha\beta}$.
The matrix $VU$ is also interesting.
It has a block diagonal structure, with each block corresponding to a subset.
Each block is a discrete-time ergodic Markov matrix (it is an independent trials process with probabilities given by the measure chosen for $U$).
This means that the right eigenvectors with eigenvalue 1 will be those that are constant in each subset:
%
\begin{equation}\label{eq:setconst}
  VU\mathbf{x}=\mathbf{x}
  \qquad\means\qquad
  \mathbf{x} = \sum_\alpha x_\alpha \onev^\alpha.
\end{equation}
%

This allows us to write the lumpability condition \cref{eq:lump}, and the transition matrix for the lumped process compactly:
%
\begin{equation}\label{eq:lumpcompact}
  VU\MM V = \MM V,
  \qquad
  \widehat{\MM} = U\MM V.
\end{equation}
%
By induction, one can show that similar relations hold for all powers:
%
\begin{equation}\label{eq:lumppower}
  VU\MM^nV = \MM^nV,
  \qquad
  \widehat{\MM}^n = U\MM^n V,
\end{equation}
%
and, via the Taylor series, for the exponential as well:
%
\begin{equation}\label{eq:lumpexp}
  VU \e^{t\MM} V = \e^{t\MM}V,
  \qquad
  \e^{t\widehat{\MM}} = U\e^{t\MM} V.
\end{equation}
%
The equilibrium distribution of the lumped process is given by
%
\begin{equation}\label{eq:lumpeq}
  \widehat{\mathbf{p}}^\infty = \eq V.
\end{equation}
%


\section{Signal-to-Noise ratio (SNR)}\label{sec:SNR}

In this section we will look at the signal-to-noise curve, and put an upper bound on its initial value.
We need only consider ergodic Markov chains.
Transient states would be unoccupied in equilibrium and would not be accessed by the signal creation process, therefore they could be removed from the analysis.
Absorbing chains are degenerate cases: they have zero initial signal but infinite decay times, so they can only be approached as the limit of a sequence of ergodic chains.

\subsection{Framework}\label{sec:framework}

The individual potentiation/depression events will be described by \emph{discrete}-time Markov chains:
%
\begin{equation}\label{eq:MWdef}
  \M\potdep \equiv \I + \W\potdep,
  \qquad
  \M\potdep\onev = \onev,
  \qquad
  \M\potdep_{ij} \in [0,1].
\end{equation}
%
The initial signal creation event occurs at time $t=0$, but all subsequent potentiation/depression events occur at random times according to Poisson processes with rates $rf\potdep$, where $f\pot +f\dep =1$ are the fraction of plasticity events that are potentiating/depressing respectively.
This means that the ``forgetting'' process will be described by the \emph{continuous}-time Markov chain:
%
\begin{equation}\label{eq:forgetting}
  \MM = r \frg \equiv r\prn{f\pot\W\pot + f\dep\W\dep}.
\end{equation}
%
We only require that this Markov chain is ergodic.
The Markov chains described by $\M\potdep $ need not be.


We assume that the probability distribution starts in the equilibrium distribution \cref{eq:equilibrium}.
During the initial signal creation, a fraction $f\pot$ will change to $\eq\M\pot$ and a fraction $f\dep$ will change to $\eq\M\dep$.
After this, probabilities will evolve according to \cref{eq:rowsum}.

\subsection{SNR curve}\label{sec:SNRcurve}

As discussed in \main, the signal-to-noise ratio is given by
%
\begin{equation}\label{eq:SNRdef:s}
  \snr(t) = \frac{\av{\wv_\text{ideal}\cdt\wv(t)} - \av{\wv_\text{ideal}\cdt\wv(\infty)}}
     {\sqrt{\var(\wv_\text{ideal}\cdt\wv(\infty))}}.
\end{equation}
%
First, let's look at the denominator, remembering that the states and plasticity events of each synapse are independent and identically distributed:
%
\begin{equation}\label{eq:noise}
\begin{aligned}
  \var(\wv_\text{ideal}\cdt\wv(\infty))
    &= \sum_{\alpha\beta} \av{\wv^\alpha_\text{ideal}\wv^\alpha(\infty) \wv^\beta_\text{ideal}\wv^\beta(\infty)}
    - \prn{\sum_\alpha \av{\wv^\alpha_\text{ideal}\wv^\alpha(\infty)}}^2 \\
    &= \sum_{\alpha} \av{(\wv^\alpha_\text{ideal})^2(\wv^\alpha(\infty))^2}
    + \sum_{\alpha\neq\beta} \av{\wv^\alpha_\text{ideal}\wv^\alpha(\infty)}\av{\smash{\wv^\beta}_\text{ideal}\smash{\wv^\beta}(\infty)}
%    \\&\phantom{= \sum_{\alpha}  \av{(\wv^\alpha_\text{ideal})^2(\wv^\alpha(\infty))^2}}
    - \prn{\sum_\alpha \av{\wv^\alpha_\text{ideal}\wv^\alpha(\infty)}}^2  \\
    &= N\av{1}
    + N(N-1)\av{\wv^1_\text{ideal}\wv^1(\infty)}^2
    - N^2\av{\wv^1_\text{ideal}\wv^1(\infty)}^2 \\
    &= N(1-\av{\wv^1_\text{ideal}\wv^1(\infty)}^2),
\end{aligned}
\end{equation}
%
where we used $\wv^\alpha=\pm1$.

For the numerator, we can write
%
\begin{equation}\label{eq:overlapex}
  \av{\wv_\text{ideal}\cdt\wv(t)} = \sum_\alpha\av{\wv^\alpha_\text{ideal}\wv^\alpha(t)}
   = N \av{\wv^1_\text{ideal}\wv^1(t)},
\end{equation}
%
Noting that $\wvi=\pm1$ with probability $f\potdep$,
%
\begin{equation}\label{eq:overlapstate}
\begin{aligned}
  \av{\wv^1_\text{ideal}\wv^1(t)} &= f\pot \av{\wv^1(t)}_{\text{pot},t=0} - f\dep \av{\wv^1(t)}_{\text{dep},t=0} \\
   &= f\pot \sum_i P(\text{state}=i,t \mid \text{pot},0) \w_i - f\dep \sum_i P(\text{state}=i,t \mid \text{dep},0) \w_i.
\end{aligned}
\end{equation}
%
From the previous section,
%
\begin{equation}\label{eq:stateprob}
  P(\text{state}=i,t \mid \text{pot/dep},0) = \brk{ \eq \M\potdep\, \e^{rt\frg} }_i,
\end{equation}
%
which describes the synapses starting in the equilibrium distribution, changing state due to the plasticity event at $t=0$ and subsequent evolution according to \cref{eq:rowsum} due to plasticity events uncorrelated with $\wvi$.%
\footnote{Note that expanding the exponential gives
%
\begin{equation*}
  \e^{rt\frg} =  \sum_{n=0}^\infty \frac{(rt)^n\,\e^{-rt}}{n!} \sum_{m=0}^n (f\pot)^m (f\dep)^{n-m}
  \brk{ \M\pot \M\dep \M\pot \M\pot \ldots + \text{permutations}}.
\end{equation*}
%
Thus, evolving according to \cref{eq:rowsum} results in averaging over all sequences of plasticity events,
as we only need linear expectations of $\wv(t)$ in the end.\label{ft:avexp}}
This results in
%
\begin{equation}\label{eq:overlapwt}
\begin{aligned}
  \av{\wv^1_\text{ideal}\wv^1(t)} &= \eq (f\pot\M\pot-f\dep\M\dep)\, \e^{rt\frg} \w, \\
  \av{\wv^1_\text{ideal}\wv^1(\infty)} &= \eq (f\pot\M\pot-f\dep\M\dep)\, \onev\eq \w \\
         &= \eq (f\pot\onev-f\dep\onev)\, \eq \w \\
         &=  (f\pot-f\dep)\, \eq \w \\
         &=  (f\pot-f\dep)\, \eq \e^{rt\frg} \w .
\end{aligned}
\end{equation}
%
Combining these allows us to write the numerator as
%
\begin{equation}\label{eq:signal}
\begin{aligned}
  \av{\wv_\text{ideal}\cdt\wv(t)} - \av{\wv_\text{ideal}\cdt\wv(\infty)}
    &= N \eq (f\pot(\M\pot-\I)-f\dep(\M\dep-\I))\, \e^{rt\frg} \w \\
    &= N \eq (f\pot(\W\pot-\frg)-f\dep(\W\dep-\frg))\, \e^{rt\frg} \w \\
%    &= N \eq (f\pot(f\dep\W\pot-f\dep\W\dep)-f\dep(f\pot\W\dep-f\pot\W\pot))\,\e^{rt\frg}\w \\
    &= N (2f\pot f\dep)\eq (\W\pot  - \W\dep )\,\e^{rt\frg}\w.
\end{aligned}
\end{equation}
%
where we used $\eq\frg=0$ in going from the first to second lines.
Combining with \cref{eq:noise} gives
%
\begin{equation}\label{eq:SNRcurveExact}
  \SNR(t) = \frac{\sqrt{N} (2f\pot f\dep ) \eq (\W\pot  - \W\dep )\,\e^{rt\frg}\w}
                 {\sqrt{1-(f\pot-f\dep)^2(\eq_+-\eq_-)^2}}.
\end{equation}
%
The denominator will not play any role in what follows, as the models that maximize the various measures of memory performance all have some sort of balance between potentiation and depression, either with $f\pot=f\dep$ or $\eq_+=\eq_-$.
We can set the denominator to 1 without changing any of our results.

This results in our final formula:
%
\begin{equation}\label{eq:SNRcurve}
  \SNR(t) = \sqrt{N} (2f\pot f\dep )\, \eq (\W\pot  - \W\dep )\,\e^{rt\frg}\w.
\end{equation}
%
The factor of $\eq$ describes the synapses being in the steady-state distribution before the memory is encoded. The factor of $(\M\pot-\M\dep)$ comes from the encoding of the memory at $t=0$, with $\wvi$ being $\pm1$ in synapses that are potentiated/depotentiated. The factor of $\e^{rt\frg}$ describes the subsequent evolution of the probability distribution, averaged over all sequences of plasticity events, and the factor of $\w$ indicates the readout via the synaptic weight.

We can express this in terms of the one parameter family of transition matrices:
%
\begin{equation}\label{eq:Walpha}
  \begin{aligned}
  \W(\alpha) &= \alpha\W\pot +(1-\alpha)\W\dep ,
  &\quad&\implies&
    \frg&=\W(f\pot ),\\&&&&
    \W\pot  - \W\dep  &= \diff{\W}{\alpha},\\&&&&
    \eq \diff{\W}{\alpha} &= -\diff{\eq}{\alpha} \frg.
  \end{aligned}
\end{equation}
%
Then \cref{eq:SNRcurve} becomes
%
\begin{equation}\label{eq:SNRalpha}
  \SNR(t) = \sqrt{N} (2f\pot f\dep ) \diff{\eq}{\alpha}(-\frg)\,\e^{rt\frg}\w.
\end{equation}
%




\subsection{Lumpability and the SNR curve}\label{sec:SNRlump}

Suppose that we have a partition such that $\W\pot $ and $\W\dep $ are simultaneously lumpable, and that all the states in each subset have the same synaptic strength (see \cref{sec:lump}):
%
\begin{equation}\label{eq:lumpablesynapse}
  VU\W\potdep  V = \W\potdep  V,
  \qquad
  VU\w=\w.
\end{equation}
%
We can define a new synapse with
%
\begin{equation}\label{eq:lumpedsynapse}
  \widehat{\W}\potdep  = U\W\potdep  V,
  \qquad
  \widehat{\w} = U \w,
  \qquad
  \widehat{\mathbf{p}}^\infty = \eq V.
\end{equation}
%
This synapse has an SNR curve:
%
\begin{equation}\label{eq:lumpedSNR}
  \begin{aligned}
    \frac{\SNR(t)}{\sqrt{N} (2f\pot f\dep )} &=  \widehat{\mathbf{p}}^\infty (\widehat{\W}\pot  - \widehat{\W}\dep )\e^{rt\widehat{\W}^F}\widehat{\w}. \\
      &= \eq VU (\W\pot  - \W\dep ) VU \e^{rt\frg} VU \w. \\
      &= \eq (\W\pot  - \W\dep ) VU \e^{rt\frg} VU \w. \\
      &= \eq (\W\pot  - \W\dep )\e^{rt\frg} VU \w. \\
      &= \eq (\W\pot  - \W\dep )\e^{rt\frg}\w. \\
  \end{aligned}
\end{equation}
%
\ie the lumped process has exactly the same SNR as the original one.


\subsection{Initial SNR and flux}\label{sec:initflux}

Using $\eq\frg=0$ and the first line of \cref{eq:signal}, we can write the initial SNR as
%
\begin{equation}\label{eq:init:s}
  \frac{\SNR(0)}{\sqrt{N}} = I = ({\eq}^++{\eq}^-)(f\pot \W\pot -f\dep \W\dep )(\onev^+-\onev^-).
\end{equation}
%
Using $\W\potdep (\onev^++\onev^-)=0$ and \cref{eq:compflux}:
%
\begin{equation*}
  r{\eq}^-(f\pot \W\pot +f\dep \W\dep )\onev^+ = \F_{-+} = \F_{+-} = r{\eq}^+(f\pot \W\pot +f\dep \W\dep )\onev^-,
\end{equation*}
%
we can rewrite \cref{eq:init} as
%
\begin{equation}\label{eq:initflux:s}
  I = \frac{4\F_{-+}}{r} - 4{\eq}^+f\pot \W\pot \onev^- - 4{\eq}^-f\dep \W\dep \onev^+.
\end{equation}
%
The last two terms are guaranteed to be negative, as the diagonal parts of $\W\potdep $ cannot contribute.
Therefore
%
\begin{equation}\label{eq:initfluxineq}
  \SNR(0) \leq \frac{4\sqrt{N}\F_{-+}}{r}.
\end{equation}
%
This inequality is saturated if potentiation never takes it from a $+$ state to a $-$ state and depression never takes it from a $-$ state to a $+$ state.


\section{Area maximisation}\label{sec:areamax}

In this section we will find an upper bound on the area under the signal-to-noise curve.
As in \cref{sec:SNR}, we will only consider ergodic Markov chains.
We will see in \cref{sec:multistate} that the optimal chain is absorbing, so it lies on the boundary of the (open) set of ergodic chains, but it still puts an upper bound on the area for any chain in the interior.

\subsection{Area under signal-to-noise curve}\label{sec:areadef}

The signal-to-noise curve is given by \cref{eq:SNRalpha}.
%%
%\begin{equation}\label{eq:SNR}
%  \SNR(t) = \sqrt{N}(2f^+f\dep ) \diff{\eq}{\alpha} (-\frg) \e^{rt\frg} \w.
%\end{equation}
%%
The area is computed by integrating this
%
\begin{equation}\label{eq:area:s}
\begin{aligned}
  A &= \frac{\sqrt{N}(2f\pot f\dep )}{r} \diff{\eq}{\alpha} \brk{-\e^{rt\frg}}_0^\infty \w \\
    &= \frac{\sqrt{N}(2f\pot f\dep )}{r} \diff{\eq}{\alpha} (\I-\onev\eq) \w \\
    &= \frac{\sqrt{N}(2f\pot f\dep )}{r} \diff{\eq}{\alpha} \w.
\end{aligned}
\end{equation}
%

We can rewrite this using \cref{eq:diffpT}, with $A=\sqrt{N}(2f\pot f\dep )\hat{A}$ and $\enc_{ij} \equiv \diff{\frg_{ij}}{\alpha}=\W\pot _{ij}-\W\dep _{ij}$
%
\begin{equation}\label{eq:areaT}
  \hat{A} = \sum_{i,j,k} \eq_i \enc_{ij} (\fptb_{ik} - \fptb_{jk}) \eq_k \w_k.
\end{equation}
%

\begin{defn}[Partial mixing times]
We define the $\pm$ mixing times as
%
\begin{equation}\label{eq:mixingpm}
\begin{aligned}
  \eta^\pm_i &\equiv \sum_k \fptb_{ik} \eq_k \prn{\frac{1 \pm \w_k}{2}}
%    &&= \sum_{\set{k}{\w_k=\pm1}} \fptb_{ik} \eq_k \\
    &&= \sum_{k\in\pm} \fptb_{ik} \eq_k \\
    &= \sum_k \prn{\fund_{kk}-\fund_{ik}} \prn{\frac{1 \pm \w_k}{2}}
    &&= \sum_{k\in\pm} \prn{\fund_{kk}-\fund_{ik}} .
\end{aligned}
\end{equation}
%
We can think of $\eta^+_i$ as a measure of the ``distance'' to the $\w_k=+1$ states and $\eta^-_i$ as the ``distance'' to the $\w_k=-1$ states.
\end{defn}
Using \cref{eq:mixdef}, we can write:
%
\begin{equation}\label{eq:mixingrels}
\begin{aligned}
  \eta^+_i + \eta^-_i &= \eta,\\
  2(\eta^+_i - \eta^+_j) &= \sum_k (\fptb_{ik}-\fptb_{jk}) \eq_k \w_k
    = \sum_k (\fund_{jk}-\fund_{ik}) \w_k.
\end{aligned}
\end{equation}
%
We could arrange the states in order of decreasing $\eta^+$, which is the same as the order of increasing $\eta^-$.

We can rewrite \cref{eq:areaT} as
%
\begin{equation}\label{eq:areaEta:s}
\begin{aligned}
  \hat{A} &= 2\sum_{i,j} \enc_{ij} \eq_i (\eta^+_{i} - \eta^+_{j}) &
    &= -2\sum_{i,j} \enc_{ij} \eq_i \eta^+_{j} \\
    &= 2\sum_{i,j} \enc_{ij} \eq_i (\eta^-_{j} - \eta^-_{i}) &
    &= 2\sum_{i,j} \enc_{ij} \eq_i \eta^-_{j}.
\end{aligned}
\end{equation}
%
We can also express it in terms of the fundamental matrix \cref{eq:funddef} as
%
\begin{equation}\label{eq:areaZ}
  \hat{A} = \sum_{i,j,k,l} \enc_{ij} \arow_{l} \fund_{li} (\fund_{jk}-\fund_{ik}) \w_k
    = \arow \fund q \fund \w.
\end{equation}
%

It is also helpful to define the following quantities:
%
\begin{equation}\label{eq:areacoeffs}
  \begin{aligned}
    c_k &= \diff{\ln\eq_k}{\alpha}
      = \sum_{ij} \eq_i \enc_{ij} \prn{\fptb_{ik}-\fptb_{jk}}
      = - \prn{\eq \enc \fptb}_k
      = \frac{(\eq \enc \fund)_k}{\eq_k}, \\
    a_i &= \sum_j \enc_{ij} \eq_i (\eta^+_{i} - \eta^+_{j}),\\
    \implies
    \hat{A} &= \sum_k c_k \eq_k \w_k
      = 2\sum_i a_i.
  \end{aligned}
\end{equation}
%
Note that the optimal choice of $\w$ is $\w_k = \sgn(c_k)$.

\subsection{Derivatives \wrt \texorpdfstring{$\W\potdep $}{W(pot/dep)}}\label{sec:deriv}

In the following, we will mathematically define the classes of perturbations pictorially described in \cref{fig:perts} of \main.
In order to do so, we will need to consider expressions for derivatives of various quantities with respect to $\W\potdep_{ij}$.

As discussed in \main,
we will regard the off-diagonal elements of $\W\potdep _{ij}$ to be the independent variables,
with $\W\potdep _{ii}=-\sum_{j \neq i} \W\potdep _{ij}$ imposed by hand.
Thus,
%
\begin{equation}\label{eq:basicderivs}
  \pdiff{\frg_{ij}}{\W\potdep _{gh}} = f\potdep  \delta_{gi}(\delta_{hj}-\delta_{ij}),
  \qquad
  \pdiff{\enc_{ij}}{\W\potdep _{gh}} = \pm\delta_{gi}(\delta_{hj}-\delta_{ij}).
\end{equation}
%
The implicit $g \neq h$ that comes with all derivatives is unnecessary, as the derivatives above vanish when $g=h$.

In particular, differentiating \cref{eq:funddef},
%
\begin{equation}\label{eq:derivZ}
  \pdiff{\fund_{ij}}{\W\potdep _{gh}} = rf\potdep  \fund_{ig} (\fund_{hj}-\fund_{gj}).
\end{equation}
%
We can then differentiate expression \cref{eq:areaZ} to get
%
\begin{equation}\label{eq:derivA}
  \begin{aligned}
    \pdiff{\hat{A}}{\W\potdep _{gh}} =&\,
%      rf\potdep  \sum_{ijkl} \enc_{ij} \w_k \arow_l \brk{
%         \fund_{lg}(\fund_{hi}-\fund_{gi})(\fund_{jk}-\fund_{ik}) + \fund_{li}(\fund_{jg}-\fund_{ig})(\fund_{hk}-\fund_{gk}) } \\
%    &\pm \sum_{kl} \w_k \arow_l \fund_{lg} (\fund_{hk} - \fund_{gk}) \\
%    =&\,
%      2rf\potdep  \sum_{ij} \enc_{ij} \eq_i \eq_g \brk{
%         (\fptb_{gi}-\fptb_{hi})(\eta^+_{i}-\eta^+_{j}) +
%         (\fptb_{ig}-\fptb_{jg})(\eta^+_{g}-\eta^+_{h}) } \\
%      &\pm 2 \eq_g (\eta^+_g - \eta^+_h) \\
%    =&\,
%      2rf\potdep  \sum_{ij} \enc_{ij} \eq_i \eq_g
%         (\fptb_{gi}-\fptb_{hi})(\eta^+_{i}-\eta^+_{j})
%      \pm 2 \eq_g (\eta^+_g - \eta^+_h)\brk{1+\pdiff{\ln(\eq_g)}{\,\ln(f\potdep )}} \\
%    =&\,
      2rf\potdep  \eq_g \brk{\sum_i a_i (\fptb_{gi}-\fptb_{hi}) + c_g (\eta^+_{g}-\eta^+_{h})}
      \pm 2 \eq_g (\eta^+_g - \eta^+_h). \\
  \end{aligned}
\end{equation}
%
where $a_i$ and $c_k$ were defined in \cref{eq:areacoeffs}.

It is sometimes useful to consider the following derivatives:
%
\begin{equation}\label{eq:derivqw}
  \begin{aligned}
    \pdiff{}{\frg_{gh}} &\equiv     \pdiff{}{\W\pot _{gh}} +     \pdiff{}{\W\dep _{gh}} ,&\qquad
    \pdiff{}{\enc_{gh}}    &\equiv f\dep  \pdiff{}{\W\pot _{gh}} - f\pot  \pdiff{}{\W\dep _{gh}} .
  \end{aligned}
\end{equation}
%
Each of these derivatives behaves as their names suggest:
%
\begin{equation}\label{eq:derivqweff}
  \pdiff{\frg_{ij}}{\frg_{gh}} = \pdiff{\enc_{ij}}{\enc_{gh}}
  =  \delta_{gi}(\delta_{hj}-\delta_{ij}),
  \qquad
  \pdiff{\enc_{ij}}{\frg_{gh}} = \pdiff{\frg_{ij}}{q_{gh}}  = 0.
\end{equation}
%
This is because we could treat $\frg$ and $q$ as the independent variables. However, the boundaries of the allowed region are more easily expressed in terms of $\W\potdep $.


\subsubsection{Scaling mode}\label{sec:rescale}

Consider the following differential operator:
%
\begin{equation}\label{eq:scaleop}
  \Delta \equiv \sum_{g,h} \W\pot _{gh}\pdiff{}{\W\pot _{gh}} + \W\dep _{gh}\pdiff{}{\W\dep _{gh}}.
\end{equation}
%
This corresponds to the scaling, $\W\potdep  \to (1+\epsilon)\W\potdep $.
Intuitively, this has two effects: it scales up the initial potentiation/depression and it scales down all timescales.
This intuition is confirmed by the following results:
%
\begin{equation}\label{eq:scaleeffects}
  \begin{aligned}
    \Delta \fund &= \tau\onev\eq - \fund ,&
    \Delta \eq  &= 0 ,&
    \Delta \fpt  &= -\fpt ,
    \\
    \Delta \eta^\pm_i  &= - \eta^\pm_i ,&
    \Delta \enc_{ij} &= \enc_{ij} ,&
    \Delta \hat{A}  &= 0 ,&
  \end{aligned}
\end{equation}
%
The anomalous bit in the scaling of $\fund$ is due to the lack of dependence of $\arow$ and $\tau$ on $\W\potdep $.

As the area is invariant under this scaling, we can consider the $\W\potdep $ to be projective coordinates.
Therefore we don't need to enforce the lower bound on the diagonal matrix elements while looking for the maximum area, as we can use this null-mode to enforce it afterwards without changing the area.
%We also don't have to worry about the boundary at infinity, as sending some of them to infinity is equivalent to sending the rest to zero.

\subsection{Kuhn-Tucker conditions}\label{sec:kuhntucker}

Consider the Lagrangian
%
\begin{equation}\label{eq:lagrangian}
  \CL = \hat{A} + \sum_{\text{pot/dep}}\sum_{i\neq j} \mu\potdep _{ij} \W\potdep _{ij} .%  +\lambda\eq\w.
\end{equation}
%
%The last term ensures that we hold $\eq_\pm$ fixed during this extremisation, so that we can safely ignore the factors that we dropped from the noise in \cref{eq:noisepm}.
Necessary conditions for an extremum are
%
\begin{equation}\label{eq:extremum}
  \pdiff{\CL}{\W\potdep _{gh}} = 0,
  \qquad
    \mu\potdep _{gh} \geq 0,\quad
    \W\potdep _{gh} \geq 0,\quad
    \mu\potdep _{gh}\W\potdep _{gh} = 0.
\end{equation}
%
with $g \neq h$. This enforces the positivity constraints on the off-diagonal elements, but not the diagonals. As discussed in \cref{sec:rescale}, that can be enforced after finding the maximum using the null scaling degree of freedom.

\subsubsection{Triangularity}\label{sec:triangular}

Here we describe the perturbations corresponding to \cref{fig:perts}(\ref{fig:tri_right},\ref{fig:tri_left}) of \main.

Consider
%
\begin{equation}\label{eq:shiftqderiv}
  \pdiff{\CL}{\enc_{gh}} =
  f\dep  \pdiff{\CL}{\W\pot _{gh}} - f\pot  \pdiff{\CL}{\W\dep _{gh}}
   = (f\dep  \mu\pot _{gh} - f\pot  \mu\dep _{gh}) + 2\eq_g (\eta^+_g - \eta^+_h)
   = 0.
\end{equation}
%
This corresponds to the shift
%
\begin{equation}\label{eq:shiftq}
  \W\pot _{ij} \to \W\pot _{ij} + f\dep \epsilon_{ij},
  \qquad
  \W\dep _{ij} \to \W\dep _{ij} - f\pot \epsilon_{ij},
  \qquad
  \sum_j \epsilon_{ij} = 0,
\end{equation}
%
which leaves $\frg$ unchanged, and therefore $\eq$, $\fpt$ and $\eta^\pm$ as well.

Assume $\eta^+_g > \eta^+_h$. Then
%
\begin{equation}\label{eq:lowertriangular}
 f\dep  \mu\pot _{gh} - f\pot  \mu\dep _{gh} <0
 \qquad\implies\qquad
 \mu\dep _{gh} >0
 \qquad\implies\qquad
 \W\dep _{gh}=0.
\end{equation}
%
Similarly, if $\eta^+_g < \eta^+_h$, then
%
\begin{equation}\label{eq:uppertriangular}
 f\dep  \mu\pot _{gh} - f\pot  \mu\dep _{gh} > 0
 \qquad\implies\qquad
 \mu\pot _{gh} >0
 \qquad\implies\qquad
 \W\pot _{gh}=0.
\end{equation}
%
Thus, if we arrange the states in order of decreasing $\eta^+$, $\W\pot $ is upper-triangular and $\W\dep $ is lower triangular.

We have ignored the possibility that $\eq_g=0$, as this would imply that $\fpt_{ig}=\infty$, which would in turn imply that the Markov process is not ergodic.

%Note that if we multiply \cref{eq:shiftqderiv} by $\enc_{gh}$ and sum over $(g,h)$, we get
%%
%\begin{equation}\label{eq:stupid}
%  \hat{A} = \sum_\pm\sum_{g\neq h} \mu\potdep _{gh} \frg_{gh},
%\end{equation}
%%
%which is probably useless

\subsubsection{Shortcuts}\label{sec:shortcuts}

In this subsection we will define perturbations corresponding to \cref{fig:perts}(\ref{fig:shortcut}) of \main.

Now consider the following combinations of derivatives for $m>1$:
%
\begin{align}
%  \begin{aligned}
\label{eq:shortcutderiv}
    \widetilde{\Delta}\potdep _{g,m} &\equiv
      \brk{ \sum_{k=0}^{m-1} \frac{1}{\eq_{g\pm k}} \pdiffc{}{\W\potdep _{g\pm k,g\pm(k+1)}} }
      - \frac{1}{\eq_g} \pdiffc{}{\W\potdep _{g,g\pm m}}.
%  \end{aligned}
\end{align}
%
Once again, they are only well defined if all the states have non-zero equilibrium probabilities (see the comment in \cref{sec:triangular} about this being satisfied for ergodic chains).

One can show that the equilibrium probabilities, $\eq$, are invariant under these operators \cref{eq:diffpT}:
%
\begin{equation}\label{eq:shortcutprob}
  \widetilde{\Delta}\potdep _{g,m} \eq_i = 0,
\end{equation}
%
which makes it possible to integrate the perturbation:
%
\begin{equation}\label{eq:shortcutfinite}
  \W\potdep  \to \W\potdep  + \D\boldsymbol{\epsilon}^{\pm(g,m)},
  \qquad
  \begin{aligned}
    &\prn{\boldsymbol{\epsilon}^{\pm(g,m)}}_{g,g\pm m}
      &\!\!\!\!\!\!&= -\epsilon,\\
    &\prn{\boldsymbol{\epsilon}^{\pm(g,m)}}_{g\pm k,g\pm(k+1)}
      &\!\!\!\!\!\!&= \epsilon
        &\;
        &\forall\, k \in [0,m-1],\\
    &\prn{\boldsymbol{\epsilon}^{\pm(g,m)}}_{g\pm k,g\pm k}
      &\!\!\!\!\!\!&= -\epsilon
        &
        &\forall\, k \in [1,m-1].
  \end{aligned}
\end{equation}
%
But more interestingly for our purposes:
%
\begin{align}
%  \begin{aligned}
\label{eq:shortcutarea}
    \widetilde{\Delta}\potdep _{g,m}\CL &=
      \brk{ \sum_{k=0}^{m-1} \frac{\mu\potdep _{g\pm k,g\pm(k+1)}}{\eq_{g\pm k}}
      - \frac{\mu\potdep _{g,g\pm m}}{\eq_g}}
      + 2rf\potdep  \sum_{k=0}^{m-1} \prn{\eta^+_{g\pm k} - \eta^+_{g\pm(k+1)}} \prn{c_{g\pm k} - c_{g}},
%  \end{aligned}
\end{align}
%

In the \hyperref[sec:areacoeffincr]{\lcnamecref{sec:areacoeffincr} below}, we will show that the $c_k$ are non-decreasing, if we put the states in order of decreasing $\eta^+_k$.
This implies that the last term of the final expression in \cref{eq:shortcutarea} is non-negative.
If it is non-zero (there would need to be a lot of degeneracy for it to be zero), this would imply that $\mu\potdep _{g,g\pm m}>0$, which in turn implies that $\W\potdep _{g,g\pm m}=0$.
This would tell us that the process with the maximal area has to have a multi-state topology.

\subsubsection{Increasing \texorpdfstring{$c_k$}{c(k)}}\label{sec:areacoeffincr}

In the \hyperref[sec:shortcuts]{previous \lcnamecref{sec:shortcuts}} we defined perturbations corresponding to \cref{fig:perts}(\ref{fig:shortcut}) of \main.
In order to show that those perturbations increase the area, we must now show that the $c_k$ are non-decreasing, if we put the states in order of decreasing $\eta^+_k$.

Consider the following combinations of derivatives:
%
\begin{align}
%  \begin{aligned}
\label{eq:areacoeffincrderiv}
    \Delta_{gh} &\equiv
      \frac{1}{\eq_{g}} \pdiffc{}{\frg_{gh}}
      + \frac{1}{\eq_h} \pdiffc{}{\frg_{hg}}, \\
%  \end{aligned}
\end{align}
%
Note that they are only well defined if all the states have non-zero equilibrium probabilities (see the comment in \cref{sec:triangular} about this being satisfied for ergodic chains).

One can show that the equilibrium probabilities, $\eq$, are invariant under these operators using \cref{eq:diffpT}:
%
\begin{equation}\label{eq:sareacoeffincrprob}
  \Delta_{gh} \eq_i = 0,
\end{equation}
%
which makes it possible to integrate the perturbation:
%
\begin{equation}\label{eq:areacoeffincrfinite}
  \W\potdep  \to \W\potdep  + \D\boldsymbol{\epsilon},
  \qquad
  \begin{aligned}
  \boldsymbol{\epsilon} &= \boldsymbol{\epsilon}\trans,
  \\
  \boldsymbol{\epsilon} \onev &= 0.
  \end{aligned}
\end{equation}
%
But more interestingly:
%
\begin{align}
%  \begin{aligned}
\label{eq:areacoeffincr}
    \Delta_{gh}\CL &=
      \frac{\mu\pot _{gh}+\mu\dep _{gh}}{\eq_g} + \frac{\mu\pot _{hg}+\mu\dep _{hg}}{\eq_h}
      + 2r\prn{c_g-c_h}\prn{\eta^+_{g} - \eta^+_{h}},\\
%  \end{aligned}
\end{align}
%
where $c_k$ were defined in \cref{eq:areacoeffs}.

Using the non-negativity of the Kuhn-Tucker multipliers, $\mu\potdep _{ij}$, \cref{eq:areacoeffincr} tells us that if we arrange the states in order of decreasing $\eta^+_i$, the optimal process will have non-decreasing $c_k$ (if any of the $\eta^+_k$ are degenerate, we can choose their order to ensure this).

Note that, according to \cref{sec:triangular}, either $\W\pot _{gh}$ or $\W\dep _{gh}$ will be zero at the maximum, therefore we can expect one of $\mu\pot _{gh}+\mu\dep _{gh}$ to be non-zero.
This would rule out degeneracy of the $c_k$ or $\eta^+_k$.
Looking at \cref{eq:shiftqderiv} closely, the only way $\mu\pot _{gh}+\mu\dep _{gh}$ could be zero is if $\eta^+_g=\eta^+_h$ or $\eq_g=0$.


%From the expression for the area in \cref{eq:areacoeffs}, it will clearly also be optimal to have the $\w_k$ non-decreasing, \ie all the positive $\w_k$ should lie ahead of all the negative $\w_k$.

\subsubsection{Summary}\label{sec:KTsummary}

Using the Kuhn-Tucker formalism, we have shown that,
with the states arranged in order of non-increasing $\eta^+_i$:
%
\begin{itemize}
  \item There can be no ergodic maximum for which $\W\pot $ contains backwards transitions or $\W\dep $ contains forwards transitions.
  \item There can be no ergodic maximum with the $c_k$ decreasing.
  \item The $c_k$ may only be degenerate at an ergodic maximum if the corresponding $\eta^+_k$ are also degenerate.
  \item If the $c_k$ increase and the $\eta^+_i$ decrease, there can be no ergodic maximum with shortcuts.
\end{itemize}
%
These were shown by finding allowed perturbations that increase the area.

%Suppose we start with some arbitrary ergodic process. If $\W\potdep $ contain backwards/forwards transitions or the $c_k$ decrease, then we can increase the area by applying the perturbations \cref{eq:shiftqderiv} and \cref{eq:areacoeffincrderiv}.
%The second perturbation will generate the unwanted backwards/forwards transitions, requiring subsequent application of first perturbation.
%The first perturbation will change the $c_k$, possibly changing their order, requiring subsequent application of second perturbation.
%
%This procedure can end in several ways:
%%
%\begin{itemize}
%  \item Applying \cref{eq:areacoeffincrderiv} an infinite amount sends two transition rates to infinity, which due to the scaling mode (\cref{sec:rescale}), is equivalent to sending all other transitions to zero.
%      Subsequent application of \cref{eq:shiftqderiv} just removes the backwards transition from $\W\pot $ and the forward transition from $\W\dep $.
%  \item Applying \cref{eq:areacoeffincrderiv} ends in a process with degenerate $c_k$ or $\eta^+_k$, and the subsequent application of \cref{eq:shiftqderiv} doesn't change this degeneracy.
%  \item As above, but the subsequent application of \cref{eq:shiftqderiv} lifts the degeneracy, resulting in a chain with increasing $c_k$.
%\end{itemize}
%%
%In the third case, we can then apply the perturbation \cref{eq:shortcutderiv} to remove all the shortcuts.
%Either this, or the first case, results in a multi-state topology, possibly disconnected or without the full set of states.

This leaves two possibilities for the maximum area Markov chain.
Either there is no degeneracy and no shortcuts, which implies the Multi-state/serial topology that we'll discuss in \cref{sec:multistate}, or there is some degeneracy, which would allow shortcuts provided that they do not bypass an entire degenerate set (see \cref{eq:shortcutarea}).

Degeneracy tends to be very delicate. It is usually hard to arrange without some symmetry relating degenerate states. Such a symmetry would imply lumpability (see \cref{sec:lump}). The lumped chain would not have any shortcuts, as an entire degenerate set cannot be bypassed. As this lumped chain has the same area (see \cref{sec:SNRlump}), we would need only consider the multi-state topology.


\subsection{Multi-state/Serial topology}\label{sec:multistate}

The previous results indicate that the area under the memory curve of any model is bounded by the area under the memory curve of a model with the serial/multistate topology having the same equilibrium distribution.
Here we compute this area, which we will see depends only on this equilibrium distribution.

The multi-state/serial topology is defined by (see \cite{amit1994learning,Fusi2007multistate,Leibold2008serial}):
%
\begin{equation}\label{eq:multistatedef}
  \W\pot _{ij} = q\pot _i (\delta_{i+1,j} - \delta_{ij}),
  \qquad
  \W\dep _{ij} = q\dep _j (\delta_{i,j+1} - \delta_{ij}).
\end{equation}
%
Because it has no shortcuts, it saturates various inequalities:
%
\begin{equation}\label{eq:multistateineq}
  \begin{aligned}
    \fptb_{ik} - \fptb_{jk} &=
      \begin{cases}
        \fptb_{ij},  &\text{if}\quad i \leq j \leq k \quad\text{or}\quad i \geq j \geq k,\\
        -\fptb_{ji}, &\text{if}\quad j \leq i \leq k \quad\text{or}\quad j \geq i \geq k,\\
      \end{cases} \\
    r\eq_i \frg_{ij} \prn{\fptb_{ij}+\fptb_{ji}} &= 1 \quad\text{if}\quad i=j\pm1,
  \end{aligned}
\end{equation}
%
and it satisfies detailed balance (a.k.a.\ reversibility a.k.a. $\CL^2_{\eq}$ self-adjointness):
%
\begin{equation}\label{eq:multistateprob}
  f\pot  q\pot _i \eq_i = f\dep  q\dep _i \eq_{i+1},
\end{equation}
%
which means we can always choose the transition rates, $q\potdep _i$, to give any desired equilibrium probabilities, $\eq_i$.


This allows us to calculate the $c_k$'s:
%
\begin{equation}\label{eq:areacoeffchain}
\begin{aligned}
  c_k =&\, \sum_{i<k} \fpt_{i,i+1} \prn{\eq_{i}q\pot _i+\eq_{i+1}q\dep _i}
    %\\&
    - \sum_{i\geq k} \fpt_{i+1,i} \prn{\eq_{i}q\pot _i+\eq_{i+1}q\dep _i}
  ,\\
  c_{k+1} - c_k =&\, \prn{\fpt_{k,k+1}+\fpt_{k+1,k}} \prn{\frac{\eq_{k}\frg_{k,k+1}}{f\pot }+\frac{\eq_{k+1}\frg_{k+1,k}}{f\dep }}
    =%\\=&\,
    \frac{1}{rf\pot f\dep },\\
  \sum_k c_k \eq_k =&\, \sum_{ij} \eq_i \enc_{ij} (\eta-\eta) = 0,\\
%  \implies c_k =&\, \frac{\prn{k-\frac{n+1}{2}} - \sum_j\prn{j-\frac{n+1}{2}}\eq_j}{f\pot f\dep },
  \implies c_k =&\, \frac{k - \sum_j j\eq_j}{rf\pot f\dep },
\end{aligned}
\end{equation}
%
where we used \cref{eq:multistateineq} to derive the first two equations respectively and \cref{th:kemenyconst} to derive the third. This allows us to write the area as
%
\begin{equation}\label{eq:multistatearea:s}
 % A = 2\sqrt{N} \sum_k \brk{\prn{k-\frac{n+1}{2}} - \sum_j\prn{j-\frac{n+1}{2}}\eq_j} \eq_k \w_k.
  A = \frac{2\sqrt{N}}{r} \sum_k \brk{k - \sum_j j\eq_j} \eq_k \w_k
    = \frac{2\sqrt{N}}{r} \sum_k \abs{k - \sum_j j\eq_j} \eq_k ,
\end{equation}
%
where we used $\w_k=\sgn(c_k)$, as discussed after \cref{eq:areacoeffs}.
This reproduces \cref{eq:multistatearea} of \main.

%It will also help to define $\eq_\pm = \sum_k \eq_k \prn{\frac{1\pm\w_k}{2}}$.
%Note that
%%
%\begin{equation}\label{eq:allpplusorminus}
%  \begin{aligned}
%    \eq_+ &=1
%     \quad&\implies\quad
%     \eq_k \w_k &= \eq_k
%     \quad&\implies\quad
%     A=0,\\
%    \eq_- &=1
%     \quad&\implies\quad
%     \eq_k \w_k &= -\eq_k
%     \quad&\implies\quad
%     A=0,
%  \end{aligned}
%\end{equation}
%%
%neither of which are optimal.
%
%Consider the Lagrangian
%%
%\begin{equation}\label{eq:multistatelagrangian}
%  \CL = \frac{A}{2\sqrt{N}} + \lambda\prn{1-\sum_i \eq_i} + \sum_i \mu_i \eq_i,
%\end{equation}
%%
%where $\lambda$ is a Lagrange multiplier and $\mu_i$ are Kuhn-Tucker multipliers (see \cref{eq:extremum}). Extremising \wrt $\eq_i$:
%%
%\begin{equation}\label{eq:multistateext}
%%  \pdiff{\CL}{\eq_i} = \brk{\prn{i-\frac{n+1}{2}} - \sum_j\prn{j-\frac{n+1}{2}}\eq_j} \w_i
%%    - \prn{i-\frac{n+1}{2}}\prn{\eq_+-\eq_-} - \lambda + \mu_i = 0.
%  \pdiff{\CL}{\eq_i} = \brk{i - \sum_j j\eq_j} \w_i
%    - i\prn{\eq_+-\eq_-} - \lambda + \mu_i = 0.
%\end{equation}
%%
%Consider an $i$ such that $\w_i=\w_{i+1}$:
%%
%\begin{equation}\label{eq:multistatemudiff}
%  \mu_{i+1} - \mu_i = \prn{\eq_+-\eq_-} - \w_i.
%\end{equation}
%%
%As $\abs{\eq_+-\eq_-}<1$ (unless we want $A=0$, \cref{eq:allpplusorminus}), this means the $\mu_i$ increase amongst the negative $\w_i$ and decrease amongst the positive $\w_i$.
%Therefore the only non-zero equilibrium probabilities at optimum are $\eq_1$ and $\eq_n$.

In order to obtain an upper bound on the area under the memory curve of any model, we now maximise the area of the serial model with respect to its equilibrium distribution.
First let us maximise \cref{eq:multistatearea:s} at fixed $\eq_\pm = \sum_k \eq_k \prn{\frac{1\pm\w_k}{2}}$.
Clearly this will happen when we put all of the probability at the ends: $\eq_1=\eq_-$ and $\eq_n=\eq_+$ are the only non-zero $\eq_k$.
This gives an area of
%\footnote{Note that including the dropped factors in \cref{eq:noisepm} would only change this to $A \leq \sqrt{N(4\eq_+\eq_-)}(n-1)/r$, which would have no effect on what follows.}

%
\begin{equation}\label{eq:multistateextarea}
  A \leq \frac{\sqrt{N}}{r}(M-1)\prn{4\eq_+\eq_-}.
\end{equation}
%
This is maximised at $\eq_+=\eq_-=\half$:
%
\begin{equation}\label{eq:maxarea}
  A \leq \frac{\sqrt{N}}{r}(M-1).
\end{equation}
%
This yields the area bound of equation \cref{eq:max_area} of \main.

Note that the chain that achieves this is not ergodic, the two states at each end are absorbing. This is similar to the results found numerically in \cite{Barrett2008discrete} in a slightly different situation.

\end{strip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \pnasbreak splits and balances the columns before the references.
% If you see unexpected formatting errors, try commenting out this line
% as it can run into problems with floats and footnotes on the final page.
%\pnasbreak

%\bibliographystyle{utcaps_sl}
\begingroup
\renewcommand{\bibitem}[1]{\item[\citenum{#1}.]}
\bibliography{maths,neuro,markov}
\endgroup


\end{document}


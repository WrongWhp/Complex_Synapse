% -*- TeX -*- -*- UK -*-
% ----------------------------------------------------------------
% arXiv Paper ************************************************
%
% Subhaneil Lahiri's template
%
% Before submitting:
%    Comment out hyperref
%    Comment out showkeys
%    Replace \input{?.tex} with its contents
%       or include ?.tex in zip/tar file
%    Put this file, the .bbl file, any picture or
%       other additional files and natbib.sty
%       file in a zip/tar file
%
% **** -----------------------------------------------------------
\documentclass[12pt]{article}
%Preamble:
\usepackage{a4wide}
\input{sl_preamble.tex}
%
% >> Only for drafts! <<
\usepackage[notref,notcite]{showkeys}
% ----------------------------------------------------------------
%\numberwithin{equation}{section}
%\renewcommand{\baselinestretch}{1.5}
% ----------------------------------------------------------------
% New commands etc.
\input{sl_definitions.tex}
\input{sl_symbols.tex}
%matrices
\newcommand{\inv}{^{-1}}
\newcommand{\dg}{^\mathrm{dg}}
\newcommand{\trans}{^\mathrm{T}}
\newcommand{\I}{\mathbf{I}}
%vec of ones
\newcommand{\onev}{\mathbf{e}}
%mat of ones
\newcommand{\onem}{\mathbf{E}}
%Markov matrix
\newcommand{\MM}{\mathbf{Q}}
%prob distribution
\newcommand{\pr}{\mathbf{p}}
%equilibrium distribution
\newcommand{\eq}{\pr^\infty}
%first passage times
\newcommand{\fpt}{\mathbf{T}}
%off-diag first passage times
\newcommand{\fptb}{\overline{\fpt}}
%fundamental matrix
\newcommand{\fund}{\mathbf{Z}}
%other symbols
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\pib}{\boldsymbol{\pi}}
\newcommand{\Lb}{\boldsymbol{\Lambda}}
%synapse models
\newcommand{\w}{\mathbf{w}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\enc}{\mathbf{K}}
\newcommand{\frg}{\W^{\mathrm{F}}}
\newcommand{\F}{\boldsymbol{\Phi}}
%superscripts
\newcommand{\pot}{^{\text{pot}}}
\newcommand{\dep}{^{\text{dep}}}
\newcommand{\potdep}{^{\text{pot/dep}}}
%with activity indep processes
\renewcommand{\hom}{\mathbf{H}}
\newcommand{\Mh}{\widetilde{\M}}
\newcommand{\frgh}{\widetilde{\W}^{\mathrm{F}}}
\newcommand{\ench}{\widetilde{\enc}}
%eigenvectors
\newcommand{\evr}{\mathbf{u}}
\newcommand{\evl}{\boldsymbol{\eta}}
%snr curves etc
\newcommand{\syn}{\vec{w}}
\newcommand{\synid}{\syn_\text{ideal}}
\DeclareMathOperator{\SNR}{SNR}
\DeclareMathOperator{\snr}{SNR}
\newcommand{\snrb}{\overline{\snr}}
\DeclareMathOperator{\env}{Env}
\newcommand{\rh}{\hat{r}}
\newcommand{\CI}{\mathcal{I}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CA}{\mathcal{A}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\comp}{^\mathrm{c}}
% ----------------------------------------------------------------
\input{sl_theorems_preamble.tex}
% ----------------------------------------------------------------
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title info:
\title{Laplacian envelope}
%
% Author List:
%
\author{Subhaneil Lahiri
%
}

\begin{document}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
  We try to find the continuous time Markov process that has the maximal Laplace tranformed signal-to-noise curve.
\end{abstract}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of Article:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Framework}\label{sec:framework}

\subsection{Recognition memory}\label{sec:recog}


We will be trying to store patterns in a set od $N$ synaptic weights, $\syn$.
Every time we try to store a pattern, these synapses are subjected to a plasticity event where each synapse is either potentiated or depressed, depending on the pattern.
we will assume that these patterns are spatially and temporally independent.

At some time, suppose we wish to determine if a given pattern is one of those that we previously attempted to store.
We wish to answer this question by looking at the synaptic weights directly (ideal observer).
For that given pattern there will be an ideal set of synaptic weights, $\synid$, where those synapses that were supposed to be potentiated are maximised and those that were supposed to be depressed are minimised.
Suppose that the given pattern was actually seen at time 0 and we are observing the synapses at time $t$.
The actual set of synaptic weights we see, $\syn(t)$, will be a vector of random variables that differs from $\synid$ due to the stochasticity of the pattern encoding and all of the other (uncorrelated) pattern that are stored after it.
As $t\to\infty$, the synaptic weights will become independent of the patter stored at $t=0$.
Thus, the vector of random variables $\syn(\infty)$ also describes the synaptic weights under the null hypothesis -- if that given pattern had never been stored.

We can test if the pattern had been previously stored by computing $\synid\cdt\syn$ and comparing it to some threshold.
For large $N$, this quantity will have a Gausssian distribution.
There will be a ROC curve as a function of this threshold:
%
\begin{equation}\label{eq:ROC}
  \begin{aligned}
  \operatorname{TPR} &= \Phi_c \prn{ \frac{ \Phi_c\inv(\operatorname{FPR}) - \snr }{ \operatorname{NNR} } },
  \quad\text{where } &
    \Phi_c(x) &= \int_x^\infty \frac{ \e^{-\frac{z^2}{2}} }{ \sqrt{2\pi} }\, \dz, \\&&
    \snr &= \frac{ \av{\synid\cdt\syn(t)} - \av{\synid\cdt\syn(\infty)} }{ \sqrt{\var(\synid\cdt\syn(\infty))}}, \\&&
    \operatorname{NNR} &= \sqrt{\frac{ \var(\synid\cdt\syn(t)) }{ \var(\synid\cdt\syn(\infty)) }},
  \end{aligned}
\end{equation}
%
and TPR/FPR are the true/false positive rates.
When the signal-to-noise ratio, $\snr$ is larger than $\Phi_c\inv(\operatorname{FPR})$, it is beneficial to decrease the noise-to-noise ratio, NNR.
In the other case, it is beneficial to increase it.
The expectations and variances are over the probability distribution of the synaptic states given the sequence of plasticity events, the probability distribution of the sequence of plasticity events themselves and the probability distribution of the pattern we are testing, $\synid$.

The formulae above assumed that we know the time between storage and recognition.
We should also compute the expectations and variances over probability distribution of the recall time, $t$, as well.
If we only know average time, $\tau$, a natural choice for this distribution is
%
\begin{equation}\label{eq:recogtime}
  P(t\vert\tau) = \frac{\e^{-t/\tau}}{\tau}.
\end{equation}
%
Different parts of the brain, that store memories for different timescales, could be characterised by different values of $\tau$.


\subsection{Signal-to-noise ratio}\label{sec:snr}

We will model the synapses as having two possible synaptic weights.
As we are looking at the information contained in the synaptic weights, rather than modelling the readout via electrical activity of the neurons, what values the synaptic weights actually take is irrelevant.
It will be convenient to call these two values $\pm1$.


As discussed in \autoref{sec:recog}, the signal-to-noise ratio is given by
%
\begin{equation}\label{eq:SNRdef}
  \snr(t) = \frac{\av{\syn_\text{ideal}\cdt\syn(t)} - \av{\syn_\text{ideal}\cdt\syn(\infty)}}
     {\sqrt{\var(\syn_\text{ideal}\cdt\syn(\infty))}},
\end{equation}
%
Where the averages and variances are over the probability distribution of the synaptic states given the sequence of plasticity events, the probability distribution of the sequence of plasticity events themselves and the probability distribution of the pattern we are testing, $\synid$.
If we also average over the time between storage and recognition, we have
%
\begin{equation}\label{eq:snrb}
  \snrb(\tau) = \intd[_0^\infty]{t} P(t\vert\tau) \snr(t)
   = \frac{1}{\tau} \intd[_0^\infty]{t} \e^{-t/\tau} \snr(t),
\end{equation}
%
This is similar to the average SNR up to time $\tau$, but with the hard cut-off replaced with a smooth exponential.
This can be expressed in terms of he Laplace transform of the SNR curve in \eqref{eq:SNRdef}:
%
\begin{equation}\label{eq:laplacedef}
  A(s) = \intd[_0^\infty]{t} \e^{-st} \snr(t).
  \qquad
  \snrb(\tau) = \frac{A(1/\tau)}{\tau},
\end{equation}
%


First, let's look at the variances, remembering that the states and plasticity events of each synapse are independent and identically distributed:
%
\begin{equation}\label{eq:noise}
\begin{aligned}
  \var(\syn_\text{ideal}\cdt\syn(t))
    &= \sum_{\alpha\beta} \av{\syn^\alpha_\text{ideal}\syn^\alpha(t) \syn^\beta_\text{ideal}\syn^\beta(t)}
    - \prn{\sum_\alpha \av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}}^2 \\
    &= \sum_{\alpha} \av{(\syn^\alpha_\text{ideal})^2(\syn^\alpha(t))^2}
    + \sum_{\alpha\neq\beta} \av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}\av{\smash{\syn^\beta}_\text{ideal}\smash{\syn^\beta}(t)}
    \\&\phantom{= \sum_{\alpha}  \av{(\syn^\alpha_\text{ideal})^2(\syn^\alpha(t))^2}}
    - \prn{\sum_\alpha \av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}}^2  \\
    &= N\av{1}
    + N(N-1)\av{\syn^1_\text{ideal}\syn^1(t)}^2
    - N^2\av{\syn^1_\text{ideal}\syn^1(t)}^2 \\
    &= N(1-\av{\syn^1_\text{ideal}\syn^1(t)}^2),
\end{aligned}
\end{equation}
%
where we used $\syn^\alpha=\pm1$.
We can compute $\var(\syn_\text{ideal}\cdt\syn(\infty))$ by taking $t\to\infty$.

For the numerator, we can write
%
\begin{equation}\label{eq:overlapex}
  \av{\syn_\text{ideal}\cdt\syn(t)} = \sum_\alpha\av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}
   = N \av{\syn^1_\text{ideal}\syn^1(t)},
\end{equation}
%
If the elements of $\synid$ take values $\pm1$ with probability $f\potdep$,
%
\begin{equation}\label{eq:overlapone}
  \av{\syn^1_\text{ideal}\syn^1(t)} = f\pot \av{\syn^1(t)}_{\text{pot},t=0} - f\dep \av{\syn^1(t)}_{\text{dep},t=0}
\end{equation}
%
To compute this quantity, we will need to discuss how we model individual synapses.




\subsection{Markov models of synapses}\label{sec:markovsynapse}

We model a synapse as having $M$ internal states.
The synaptic weight depends deterministically on the state, given by the $M$-element column vector $\w$.
We denote the probability distribution across these states by the row vector $\pr(t)$.
We denote a column vector of ones by $\onev$, so the normalisation condition is $\pr(t)\onev=1$.

When the synapse is subjected to a plasticity event the internal state will change stochastically, described by the matrices $\M\potdep$ whose $i,j$ elements are the transition probabilities from state $i$ to state $j$.
In between these plasticity events the synapse will undergo stochastic transitions described by a matrix $\hom$ whose $i,j$ element is the transition rate from state $i$ to state $j$ for $i \neq j$ and $\hom_{ii}=-\sum_{j \neq i} \hom_{ij}$.
This allows us to describe homeostatic plasticity as well as plasticity processes that occur over a period of time -- $\M\potdep$ puts the synapse into some state where the process $\hom$ takes over and does the rest.
The matrices $\M\potdep$ are transition matrices of discrete time Markov processes, whereas $\hom$ is the transition matrix of a continuous time Markov process.
Their elements must satisfy the following constraints
%
\begin{equation}\label{eq:constr}
\begin{aligned}
  \M\potdep_{ij} &\geq 0, &\qquad
  \sum_j \M\potdep_{ij} &= 1, \\
  \hom_{i \neq j} &\geq 0, &\qquad
  \sum_j \hom_{ij} &= 0.
\end{aligned}
\end{equation}
%
The upper bounds $\M\potdep_{ij} \leq 1$ and $\hom_{ii} \leq 0$ follow automatically from these.
There is no upper bound on $\hom_{i \neq j}$ (or lower bound on $\hom_{ii}$) as these are rates rather than probabilities.

If we treat the off diagonal elements as the independent degrees of freedom, with the off diagonal elements determined by the row sum constraints (second column of \eqref{eq:constr}), the constraints are
%
\begin{equation}\label{eq:constri}
  \M\potdep_{i \neq j} \geq 0, \qquad
  \sum_{j\neq i} \M\potdep_{ij} \leq 1, \qquad
  \hom_{i \neq j} \geq 0.
\end{equation}
%
The second of these will be referred to as the ``diagonal'' constraint.

Given a sequence of plasticity events at times $t_1,t_2,t_3,\ldots$, the probability distribution across the internal states is
%
\begin{equation}\label{eq:plastseq}
  \pr(t_n) = \pr(t_0) \e^{\hom(t_1-t_0)} \M\pot \e^{\hom(t_2-t_1)} \M\dep \e^{\hom(t_3-t_4)} \M\dep \ldots \M\pot \e^{\hom(t_n-t_{n-1})}.
\end{equation}
%
In computing the expectation in \eqref{eq:overlapone} the plasticity event that occurs at $t=0$ is fixed, but all of the previous and subsequent events are marginalised over.
If the plasticity events occur at Poisson rate $r$, with potentiation and depression occurring independently with probability $f\potdep$, averaging over all previous and subsequent events gives
%
\begin{equation}\label{eq:plastseqav}
  P(\text{state}=i,t \mid \text{pot/dep},0) = \brk{ \eq \M\potdep\, \e^{r(f\pot\M\pot+f\dep\M\dep-\I)t +\hom t} }_i,
\end{equation}
%
where $\eq$ is the steady state distribution of the continuous time transition matrix that appears in the exponential:
%
\begin{equation}\label{eq:eq}
  \eq(r\frg +\hom) = 0,
  \qquad\text{where}\quad
  \frg = f\pot\M\pot + f\dep\M\dep - \I.
\end{equation}
%



This results in
%
\begin{equation}\label{eq:overlapwt}
\begin{aligned}
  \av{\syn^1_\text{ideal}\syn^1(t)} &= \eq (f\pot\M\pot-f\dep\M\dep)\, \e^{(r\frg+\hom)t} \w, \\
  \av{\syn^1_\text{ideal}\syn^1(\infty)} &= \eq (f\pot\M\pot-f\dep\M\dep)\, \onev\eq \w \\
         &= \eq (f\pot\onev-f\dep\onev)\, \eq \w \\
         &=  (f\pot-f\dep)\, \eq \w \\
         &=  (f\pot-f\dep)\, \eq \e^{(r\frg+\hom)t} \w .
\end{aligned}
\end{equation}
%
Combining these allows us to write the numerator of \eqref{eq:SNRdef} as
%
\begin{equation}\label{eq:signal}
\begin{aligned}
  \av{\syn_\text{ideal}\cdt\syn(t)} - \av{\syn_\text{ideal}\cdt\syn(\infty)}
    &= N \eq \enc \, \e^{(r\frg+\hom)t} \w , \\
    \text{where} \qquad
  \enc &= f\pot(\M\pot-\I)-f\dep(\M\dep-\I)
\end{aligned}
\end{equation}
%
Combining with \eqref{eq:noise} gives
%
\begin{equation}\label{eq:SNRcurveExact}
  \SNR(t) = \frac{\sqrt{N} \eq \enc\,\e^{(r\frg+\hom)t}\w}
                 {\sqrt{1-(f\pot-f\dep)^2(\eq_+-\eq_-)^2}}.
\end{equation}
%
We can absorb $\hom$ into $\MM\potdep$ by defining
%
\begin{equation}\label{eq:absorbhom}
\begin{aligned}
  \Mh\potdep &= \M\potdep + \frac{\hom}{2rf\potdep},
  &\qquad
  \frgh &= f\pot\Mh\pot + f\dep\Mh\dep - \I,
  \\&&
  \ench &= f\pot(\Mh\pot-\I)-f\dep(\Mh\dep-\I).
\end{aligned}
\end{equation}
%
Then the SNR curve is
%
\begin{equation}\label{eq:SNRcurvehom}
  \SNR(t) = \frac{\sqrt{N}  \eq \ench \,\e^{rt\frgh}\w}
                 {\sqrt{1-(f\pot-f\dep)^2(\eq_+-\eq_-)^2}}.
\end{equation}
%
The denominator will not play any role in what follows, as the models that maximize the various measures of memory performance all have some sort of balance between potentiation and depression, either with $f\pot=f\dep$ or $\eq_+=\eq_-$.
We can perform the maximisation in two steps.
First maximise the numerator at fixed $\eq\w$.
Then maximise the ratio \wrt $\eq\w$.
This will allow us to ignore the denominator until the very end.

This formula takes the same form as the one in our NIPS paper \cite{Lahiri2013synapse}, where the process $\hom$ was not included.
However, the process $\hom$ still has an impact due to the constraints.

The constraints on $\Mh\potdep$ are less stringent than the constraints on $\M\potdep$, \eqref{eq:constr}.
Clearly
%
\begin{equation}\label{eq:constrhoffdiag}
  \Mh\potdep_{i \neq j} \geq 0, \qquad
  \sum_j \Mh\potdep_{ij} = 1.
\end{equation}
%
The ``diagonal'' constraints are more complicated.
There can be many $\M\potdep,\hom$ that result in the same $\Mh\potdep$.
However, the fact that $\M\potdep_{ij} \in [0,1]$ and $\hom_{i\neq j} \geq 0$ imply that $\forall (i \neq j)$:
%
\begin{equation}\label{eq:homrange}
  2r\max\brc{f\pot(\Mh\pot_{ij}-1),f\dep(\Mh\dep_{ij}-1)} \leq \hom_{ij} \leq 2r\min\brc{f\pot\Mh\pot_{ij},f\dep\Mh\dep_{ij}}
\end{equation}
%
These two bounds are consistent when
%
\begin{equation}\label{eq:homconsistent}
  -f\dep \leq f\pot\Mh\pot_{ij} - f\dep\Mh\dep_{ij} \leq f\pot
  \qquad \forall i\neq j.
\end{equation}
%
To ensure that $\M\potdep_{ii} \geq 0$, we also need to ensure that $\sum_{j\neq i}\M\potdep_{ij} \leq 1$.
%
\begin{equation}\label{eq:homdiag}
\begin{aligned}
  \sum_{j\neq i} \M\pot_{ij}
    &= \sum_{j\neq i} \prn{ \Mh\pot_{ij} - \frac{\hom_{ij}}{2rf\pot} }\\
    &\geq \sum_{j\neq i} \prn{ \Mh\pot_{ij} - \frac{1}{f\pot}  \min\brc{ f\pot\Mh\pot_{ij},f\dep\Mh\dep_{ij} } }\\
    &= \frac{1}{f\pot} \sum_{j\neq i} \brk{ f\pot\Mh\pot_{ij}-f\dep\Mh\dep_{ij} }_+ , \\
  \sum _{j\neq i} \M\dep_{ij}
    &= \frac{1}{f\dep} \sum_{j\neq i} \brk{ f\dep\Mh\dep_{ij}-f\pot\Mh\pot_{ij} }_+ ,
\end{aligned}
\end{equation}
%
where rectification function is $[x]_\pm = x \Theta(\pm x)$, in terms of the Heaviside step function.
Demanding that these quantities are at most 1 is sufficient to ensure that \eqref{eq:homconsistent} is satisfied.

Therefore we find the set of constraints
%
\begin{equation}\label{eq:constrh}
\begin{aligned}
  \Mh\potdep_{i \neq j} &\geq 0, &\qquad
  \sum_{j\neq i} \brk{ f\pot\Mh\pot_{ij}-f\dep\Mh\dep_{ij} }_+ &\leq f\pot, \\
  \sum_j \Mh\potdep_{ij} &=1, &
  \sum_{j\neq i} \brk{ f\dep\Mh\dep_{ij}-f\pot\Mh\pot_{ij} }_+  &\leq f\dep.
\end{aligned}
\end{equation}
%
The second column of this will be referred to as the ``diagonal'' constraints.

We showed that these are necessary conditions for the existence of $\M\potdep,\hom$ satisfying \eqref{eq:constr}.
We can see that they are sufficient by choosing $\hom_{ij} = 2r\min\brc{f\pot\Mh\pot_{ij},f\dep\Mh\dep_{ij}}$ for $i \neq j$ and $\hom_{ii}=-\sum_{j \neq i} \hom_{ij}$.

We can then treat the off-diagonal elements of $\Mh\potdep$ as the independent degrees of freedom, subject to these constraints, when maximising the SNR.
The diagonal elements are determined by setting the row sums to 1.
We then have
%
\begin{equation}\label{eq:derivpd}
  \pdiff{\Mh\pot_{ij}}{\Mh\pot_{mn}{}} = \pdiff{\Mh\dep_{ij}}{\Mh\dep_{mn}{}} = \delta_{im}(\delta_{jn}-\delta_{jm}),
  \qquad
  \pdiff{\Mh\pot_{ij}}{\Mh\dep_{mn}{}} = \pdiff{\Mh\dep_{ij}}{\Mh\pot_{mn}{}} = 0.
\end{equation}
%
It will also be convenient to define the differential operators
%
\begin{equation}\label{eq:pertfe}
  \pdiff{}{\frgh_{ij}{}} = \frac{1}{2f\pot} \pdiff{}{\Mh\pot_{ij}{}} + \frac{1}{2f\dep} \pdiff{}{\Mh\dep_{ij}{}},
  \qquad
  \pdiff{}{\ench_{ij}{}} = \frac{1}{2f\pot} \pdiff{}{\Mh\pot_{ij}{}} - \frac{1}{2f\dep} \pdiff{}{\Mh\dep_{ij}{}},
\end{equation}
%
these satisfy
%
\begin{equation}\label{eq:derivfe}
  \pdiff{\frgh_{ij}}{\frgh_{mn}{}} = \pdiff{\ench_{ij}}{\ench_{mn}{}} = \delta_{im}(\delta_{jn}-\delta_{jm}),
  \qquad
  \pdiff{\frgh_{ij}}{\ench_{mn}{}} = \pdiff{\ench_{ij}}{\frgh_{mn}{}} = 0.
\end{equation}
%



\section{Laplace transform}\label{sec:laplace}

\subsection{Fundamental matrix \etc}\label{sec:lfund}

In analogy to the generalised fundamental matrix of a Markov chain \cite{Kemeny1981fund}, define
%
\begin{equation}\label{eq:lfund}
  \fund(s) = (s\I +\onev\pib -\MM)\inv,
\end{equation}
%
where $\pib$ is an arbitrary row vector satisfying $\pib\onev = \frac{1}{\tau_0} \neq 0$.
This reduces to the fundamental matrix at $s=0$.
We can see that $\onev$ is an eigenvector:
%
\begin{equation}\label{eq:lfundrowsum}
  \fund(s)\onev = \frac{\tau_0}{1+s\tau_0}\onev.
\end{equation}
%
Suppose we have some row vector $\mathbf{a}$ such that $\mathbf{a}\onev=0$.
Then $\mathbf{a}\fund(s)$ is independent of $\pib$:
%
\begin{equation}\label{eq:alfund}
  \pdiff{(\mathbf{a}\fund(s))_i}{\pib_k} = (\mathbf{a}\fund(s)\onev) \fund_{ki}(s)
      = \frac{\tau_0}{1+s\tau_0} (\mathbf{a}\onev) \fund_{ki}(s) = 0.
\end{equation}
%


Then we also define
%
\begin{equation}\label{eq:lfptb}
  \fptb(s) = (\onem\fund\dg(s)-\fund(s))\D,
  \qquad
  \fptb_{ij}(s) = \frac{\fund_{jj}(s)-\fund_{ij}(s)}{\eq_j}.
\end{equation}
%
This reduces to the first passage time matrix at $s=0$, but at other $s$ there is (probably) no relation.

Using the exact same proof as in the case $s=0$, we see that the quantity
%
\begin{equation}\label{eq:lkemeny}
  \eta(s) = \sum_j \fptb_{ij}(s)\eq_j
\end{equation}
%
is independent of the starting state $i$, but now $\eta(s) = \tr\fund(s) - \frac{\tau}{1+s\tau}$.
We can the define
%
\begin{equation}\label{eq:lpmixtime}
  \eta^{\pm}_i(s) = \sum_j \fptb_{ij}(s)\eq_j \prn{\frac{1\pm\w_j}{2}} =  \sum_{j\in\pm} \fptb_{ij}(s)\eq_j.
\end{equation}
%
We can arrange the states in order of decreasing $\eta^{+}_i(s)$ or increasing $\eta^{-}_i(s)$.
It will be more convenient to use the quantities
%
\begin{equation}\label{eq:lwpmixtime}
  \eta^w_i(s) = \sum_j \fptb_{ij}(s)\eq_j \w
    = \eta^+_i(s) - \eta^-_i(s) =2\eta^+_i(s) - \eta(s) =\eta(s) - 2\eta^-_i(s) .
\end{equation}
%
Arranging the states in order of decreasing $\eta^w_i(s)$ is the same as the order of decreasing $\eta^{+}_i(s)$ or increasing $\eta^{-}_i(s)$.
These quantities can still be used when $\w$ takes more than two values.



\subsection{Laplace transform of SNR curve}\label{sec:laplaceSNR}

Consider the Laplace transform of the evolution operator:
%
\begin{equation}\label{eq:levol}
  \mathbf{G}(s) = \intd[_0^{\infty}]{t} \e^{(\MM-s) t}.
\end{equation}
%
For $\Re s>0$, we have
%
\begin{equation}\label{eq:levolcalc}
  (s-\MM)\mathbf{G}(s) = \intd[_0^{\infty}]{t} (s-\MM)\e^{(\MM-s) t} = \brk{-\e^{(\MM-s) t}}_0^{\infty} = \I.
\end{equation}
%
As $(s-\MM)$ is invertible for $\Re s>0$, because the real part of all eigenvalues of $\MM$ are nonpositive, we have
%
\begin{equation}\label{eq:levolres}
  \mathbf{G}(s) = (s-\MM)\inv.
\end{equation}
%
For $s=0$, we can avoid problems by replacing $\mathbf{G}(s) \to \fund(s)$.

Now consider the Laplace transform of the SNR curve \eqref{eq:SNRcurvehom} (ignoring the denominator)
%
\begin{equation}\label{eq:lsnr}
\begin{aligned}
  A(s) &= \intd[_0^{\infty}]{t} \e^{-st}\SNR(t) \\
   &= \intd[_0^{\infty}]{t} \sqrt{N} \eq \ench \e^{(r\frgh-s)t} \w \\
   &= \sqrt{N} \eq \ench \prn{s - r\frgh}\inv \w.
\end{aligned}
\end{equation}
%
This expression is ill-behaved at $s=0$.
Thanks to \eqref{eq:alfund}, we can solve this by the replacement $\mathbf{G}(s) \to \fund(s)$, as $\ench\onev=0$.
%
\begin{equation}\label{eq:larea}
\begin{aligned}
  \hat{A}(s) = \frac{A(s)}{\sqrt{N}} &=  \eq \ench \fund(s) \w \\
    &= \sum_{ijk} \eq_i \ench_{ij} \prn{\fptb_{ik}(s)-\fptb_{jk}(s)} \eq_k \w_k \\
    &= \sum_{ij}  \eq_i \ench_{ij} (\eta^w_{i}(s) - \eta^w_{j}(s)).
\end{aligned}
\end{equation}
%
Note that $A(0)=A$, the area, and $\lim_{s\to\infty} \brc{sA(s)} = \SNR(0)$, the initial SNR.

In analogy with the case $s=0$, we define
%
\begin{equation}\label{eq:lareacoeffs}
  \begin{aligned}
    c_k(s) &= \sum_{ij} \eq_i \ench_{ij} \prn{\fptb_{ik}(s)-\fptb_{jk}(s)} \\
    a_i(s) &= \sum_j \eq_i \ench_{ij} (\eta^w_{i}(s) - \eta^w_{j}(s)),\\
    \implies
    \hat{A}(s) &= \sum_k c_k(s) \eq_k \w_k
      = \sum_i a_i(s).
  \end{aligned}
\end{equation}
%

\subsection{Derivatives}\label{sec:lderiv}

Using the same approach as the area bound in \cite{Lahiri2013synapse}, we find that the derivatives are
%
\begin{equation}\label{eq:lAdiff}
\begin{aligned}
  \pdiff{\hat{A}(s)}{\Mh\potdep_{mn}{}}
     =&\, rf\potdep \eq_m \brk{ \sum_i (\fptb_{mi}(0)-\fptb_{ni}(0)) a_i(s)
     +c_m(s) (\eta^w_m(s) - \eta^w_n(s)) } \\
     &\pm f\potdep \eq_m (\eta^w_m(s) - \eta^w_n(s)).
\end{aligned}
\end{equation}
%





\section{Upper bounds}\label{sec:upperbnds}



\subsection{Area bound}\label{sec:area}

The area under the SNR curve is:
%
\begin{equation}\label{eq:areadef}
  \intd[_0^\infty]{t} \snr(t) = \lim_{\tau\to\infty} \tau \snrb(\tau) = A(0).
\end{equation}
%

The only effect of $\hom$ is in the diagonal constraints (second column of \eqref{eq:constrh} vs.\ second inequality in \eqref{eq:constri}).
However, the area is invariant under the scaling mode:
%
\begin{equation}\label{eq:scale}
  \Mh\potdep  \to (1-\lambda)\I + \lambda \Mh\potdep .
\end{equation}
%
This means we can always take a set of matrices that violate either version of the diagonal constraints and use \eqref{eq:scale} to create a set of matrices that satisfy them without changing the area.
Therefore, including $\hom$ has no effect on the area bound and the result from \cite{Lahiri2013synapse} applies here as well.

This means that
%
\begin{equation}\label{eq:areabnd}
  A(0) \leq \frac{\sqrt{N}(M-1)}{r}.
\end{equation}
%
The model that saturates this bound is one of serial topology with $\eq$ concentrated symmetrically at the two end states.



\subsection{Initial SNR}\label{sec:initial}

The initial SNR is:
%
\begin{equation}\label{eq:initdef}
  \snr(0) = \snrb(0) = \lim_{s\to\infty} sA(s).
\end{equation}
%

This satisfies the inequality
%
\begin{equation}\label{eq:initbnd}
  \snr(0) \leq \sqrt{N},
\end{equation}
%
at least when $f\pot=f\dep=\frac{1}{2}$.


\subsection{Envelope}\label{sec:envelope}

Now we introduce an eigenvector decomposition for $-\frgh$:
%
\begin{equation}\label{eq:eigendecomp}
  \frgh = - \sum_a q_a \evr^a \evl^a,
  \quad
  \evl^a \evr^b = \delta_{ab},
  \quad
  \frgh \evr^a = -q_a \evr^a,
  \quad
  \evl^a \frgh = -q_a \evl^a.
\end{equation}
%
Then we can write
%
\begin{equation}\label{eqsnreig}
  \snr(t) = \sqrt{N}\sum_a \CI_a \e^{-rt/\tau_a},
  \qquad
  \snrb(\tau) = \sqrt{N}\sum_a \frac{\CI_a}{1+r\tau/\tau_a},
\end{equation}
%
where
%
\begin{equation}\label{eq:snrcoeffs}
  \CI_a = (\eq \ench \evr^a) (\evl^a \w),
  \qquad
  \tau_a = \frac{1}{q_a}.
\end{equation}
%
From \eqref{eq:areabnd} and \eqref{eq:initbnd}, we see that they are subject to the constraints
%
\begin{equation}\label{eq:coeffconstr}
  \sum_a \CI_a \tau_a \leq M-1,
  \qquad
  \sum_a \CI_a \leq 1.
\end{equation}
%
No doubt there are many other constraints that these quantities must satisfy, but we can see what these ones imply for the $\snrb(\tau)$ by maximising it subject to these constraints.

Consider the Lagrangian
%
\begin{equation}\label{eq:envlagrangian}
  \CL = \sum_a \frac{\CI_a}{1+r\tau/\tau_a} + \mu_\CI \prn{1 - \sum_a \CI_a} + \mu_\CA \prn{(M-1) - \sum_a \CI_a \tau_a}.
\end{equation}
%
The Karush-Kuhn-Tucker necessary conditions for a maximum are
%
\begin{equation}\label{eq:envKTcond}
  \pdiff{\CL}{\CI_a} = 0, \qquad
  \pdiff{\CL}{\tau_a} = 0, \qquad
  \mu_\alpha \geq 0, \qquad
  \pdiff{\CL}{\mu_\alpha} \geq 0, \qquad
  \mu_\alpha\pdiff{\CL}{\mu_\alpha} = 0.
\end{equation}
%
These are solved by
%
\begin{equation}\label{eq:envsol}
\begin{gathered}
  \CI_1 = 1, \qquad
  \CI_{a>1} = 0, \qquad
  \tau_1 = M-1, \\
  \mu_\CI = \frac{(M-1)^2}{(r\tau + (M-1))^2}, \qquad
  \mu_\CA = \frac{r\tau}{(r\tau+(M-1))^2}. \qquad
\end{gathered}
\end{equation}
%
This leads to the envelope
%
\begin{equation}\label{eq:env}
  \snrb(\tau) \leq \frac{\sqrt{N}(M-1)}{r\tau + (M-1)}.
\end{equation}
%
This is the envelope (``memory frontier'') that we can prove.
Finding more constraints would lower it.
Clearly there have to be more constraints, as \eqref{eq:envsol} indicates that this would be achieved by the same model at all times.
This model would have to saturate both the initial SNR bound \eqref{eq:initbnd} and the area bound \eqref{eq:areabnd}, whereas we saw that they were achieved by different models, which perform well at short/long timescales respectively.




















%\subsection*{Acknowledgements}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection*{Appendices}
%\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{utcaps_sl}
\bibliography{maths,neuro,markov}

\end{document}

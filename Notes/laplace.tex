% -*- TeX -*- -*- UK -*-
% ----------------------------------------------------------------
% arXiv Paper ************************************************
%
% Subhaneil Lahiri's template
%
% Before submitting:
%    Comment out hyperref
%    Comment out showkeys
%    Replace \input{?.tex} with its contents
%       or include ?.tex in zip/tar file
%    Put this file, the .bbl file, any picture or
%       other additional files and natbib.sty
%       file in a zip/tar file
%
% **** -----------------------------------------------------------
\documentclass[12pt]{article}
%Preamble:
\usepackage{a4wide}
\input{sl_preamble.tex}
%
% >> Only for drafts! <<
\usepackage[notref,notcite]{showkeys}
% ----------------------------------------------------------------
%\numberwithin{equation}{section}
%\renewcommand{\baselinestretch}{1.5}
% ----------------------------------------------------------------
% New commands etc.
\input{sl_definitions.tex}
\input{sl_symbols.tex}
%matrices
\newcommand{\inv}{^{-1}}
\newcommand{\dg}{^\mathrm{dg}}
\newcommand{\trans}{^\mathrm{T}}
\newcommand{\I}{\mathbf{I}}
%vec of ones
\newcommand{\onev}{\mathbf{e}}
%mat of ones
\newcommand{\onem}{\mathbf{E}}
%Markov matrix
\newcommand{\MM}{\mathbf{Q}}
%prob distribution
\newcommand{\pr}{\mathbf{p}}
%equilibrium distribution
\newcommand{\eq}{\pr^\infty}
%first passage times
\newcommand{\fpt}{\mathbf{T}}
%off-diag first passage times
\newcommand{\fptb}{\overline{\fpt}}
%fundamental matrix
\newcommand{\fund}{\mathbf{Z}}
%other symbols
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\pib}{\boldsymbol{\pi}}
\newcommand{\Lb}{\boldsymbol{\Lambda}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\uv}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\enc}{\mathbf{K}}
\newcommand{\frg}{\W^{\mathrm{F}}}
\renewcommand{\hom}{\mathbf{H}}
\newcommand{\Mh}{\widetilde{\M}}
\newcommand{\frgh}{\widetilde{\W}^{\mathrm{F}}}
\newcommand{\ench}{\widetilde{\enc}}
\newcommand{\F}{\boldsymbol{\Phi}}
\newcommand{\syn}{\vec{w}}
\newcommand{\synid}{\syn_\text{ideal}}
\DeclareMathOperator{\SNR}{SNR}
\DeclareMathOperator{\snr}{SNR}
\newcommand{\snrb}{\overline{\snr}}
\DeclareMathOperator{\env}{Env}
\newcommand{\rh}{\hat{r}}
\newcommand{\CI}{\mathcal{I}}
\newcommand{\CS}{\mathcal{S}}
\newcommand{\CA}{\mathcal{A}}
\newcommand{\CB}{\mathcal{B}}
\newcommand{\comp}{^\mathrm{c}}
%superscripts
\newcommand{\pot}{^{\text{pot}}}
\newcommand{\dep}{^{\text{dep}}}
\newcommand{\potdep}{^{\text{pot/dep}}}
% ----------------------------------------------------------------
\input{sl_theorems_preamble.tex}
% ----------------------------------------------------------------
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title info:
\title{Laplacian envelope}
%
% Author List:
%
\author{Subhaneil Lahiri
%
}

\begin{document}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{abstract}
  We try to find the continuous time Markov process that has the maximal Laplace tranformed signal-to-noise curve.
\end{abstract}

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beginning of Article:
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Framework}\label{sec:framework}

\subsection{Recognition memory}\label{sec:recog}


We will be trying to store patterns in a set od $N$ synaptic weights, $\syn$.
Every time we try to store a pattern, these synapses are subjected to a plasticity event where each synapse is either potentiated or depressed, depending on the pattern.
we will assume that these patterns are spatially and temporally independent.

At some time, suppose we wish to determine if a given pattern is one of those that we previously attempted to store.
We wish to answer this question by looking at the synaptic weights directly (ideal observer).
For that given pattern there will be an ideal set of synaptic weights, $\synid$, where those synapses that were supposed to be potentiated are maximised and those that were supposed to be depressed are minimised.
Suppose that the given pattern was actually seen at time 0 and we are observing the synapses at time $t$.
The actual set of synaptic weights we see, $\syn(t)$, will be a vector of random variables that differs from $\synid$ due to the stochasticity of the pattern encoding and all of the other (uncorrelated) pattern that are stored after it.
As $t\to\infty$, the synaptic weights will become independent of the patter stored at $t=0$.
Thus, the vector of random variables $\syn(\infty)$ also describes the synaptic weights under the null hypothesis -- if that given pattern had never been stored.

We can test if the pattern had been previously stored by computing $\synid\cdt\syn$ and comparing it to some threshold.
For large $N$, this quantity will have a Gausssian distribution.
There will be a ROC curve as a function of this threshold:
%
\begin{equation}\label{eq:ROC}
  \begin{aligned}
  \operatorname{TPR} &= \Phi_c \prn{ \frac{ \Phi_c\inv(\operatorname{FPR}) - \snr }{ \operatorname{NNR} } },
  \quad\text{where } &
    \Phi_c(x) &= \int_x^\infty \frac{ \e^{-\frac{z^2}{2}} }{ \sqrt{2\pi} }\, \dz, \\&&
    \snr &= \frac{ \av{\synid\cdt\syn(t)} - \av{\synid\cdt\syn(\infty)} }{ \sqrt{\var(\synid\cdt\syn(\infty))}}, \\&&
    \operatorname{NNR} &= \sqrt{\frac{ \var(\synid\cdt\syn(t)) }{ \var(\synid\cdt\syn(\infty)) }},
  \end{aligned}
\end{equation}
%
and TPR/FPR are the true/false positive rates.
When the signal-to-noise ratio, $\snr$ is larger than $\Phi_c\inv(\operatorname{FPR})$, it is beneficial to decrease the noise-to-noise ratio, NNR.
In the other case, it is beneficial to increase it.
The expectations and variances are over the probability distribution of the synaptic states given the sequence of plasticity events, the probability distribution of the sequence of plasticity events themselves and the probability distribution of the pattern we are testing, $\synid$.

The formulae above assumed that we know the time between storage and recognition.
We should also compute the expectations and variances over probability distribution of the recall time, $t$, as well.
If we only know average time, $\tau$, a natural choice for this distribution is
%
\begin{equation}\label{eq:recogtime}
  P(t\vert\tau) = \frac{\e^{-t/\tau}}{\tau}.
\end{equation}
%
Different parts of the brain, that store memories for different timescales, could be characterised by different values of $\tau$.


\subsection{Signal-to-noise ratio}\label{sec:snr}

We will model the synapses as having two possible synaptic weights.
As we are looking at the information contained in the synaptic weights, rather than modelling the readout via electrical activity of the neurons, what values the synaptic weights actually take is irrelevant.
It will be convenient to call these two values $\pm1$.


As discussed in \autoref{sec:recog}, the signal-to-noise ratio is given by
%
\begin{equation}\label{eq:SNRdef}
  \snr(t) = \frac{\av{\syn_\text{ideal}\cdt\syn(t)} - \av{\syn_\text{ideal}\cdt\syn(\infty)}}
     {\sqrt{\var(\syn_\text{ideal}\cdt\syn(\infty))}},
\end{equation}
%
Where the averages and variances are over the probability distribution of the synaptic states given the sequence of plasticity events, the probability distribution of the sequence of plasticity events themselves and the probability distribution of the pattern we are testing, $\synid$.
If we also average over the time between storage and recognition, we have
%
\begin{equation}\label{eq:snrb}
  \snrb(\tau) = \intd[_0^\infty]{t} P(t\vert\tau) \snr(t) 
   = \frac{1}{\tau} \intd[_0^\infty]{t} \e^{-t/\tau} \snr(t),  
\end{equation}
%
This is similar to the average SNR up to time $\tau$, but with the hard cut-off replaced with a smooth exponential.
This can be expressed in terms of he Laplace transform of the SNR curve in \eqref{eq:SNRdef}:
%
\begin{equation}\label{eq:laplacedef}
  A(s) = \intd[_0^\infty]{t} \e^{-st} \snr(t).
  \qquad
  \snrb(\tau) = \frac{A(1/\tau)}{\tau},
\end{equation}
%


First, let's look at the variances, remembering that the states and plasticity events of each synapse are independent and identically distributed:
%
\begin{equation}\label{eq:noise}
\begin{aligned}
  \var(\syn_\text{ideal}\cdt\syn(t))
    &= \sum_{\alpha\beta} \av{\syn^\alpha_\text{ideal}\syn^\alpha(t) \syn^\beta_\text{ideal}\syn^\beta(t)}
    - \prn{\sum_\alpha \av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}}^2 \\
    &= \sum_{\alpha} \av{(\syn^\alpha_\text{ideal})^2(\syn^\alpha(t))^2}
    + \sum_{\alpha\neq\beta} \av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}\av{\smash{\syn^\beta}_\text{ideal}\smash{\syn^\beta}(t)}
    \\&\phantom{= \sum_{\alpha}  \av{(\syn^\alpha_\text{ideal})^2(\syn^\alpha(t))^2}}
    - \prn{\sum_\alpha \av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}}^2  \\
    &= N\av{1}
    + N(N-1)\av{\syn^1_\text{ideal}\syn^1(t)}^2
    - N^2\av{\syn^1_\text{ideal}\syn^1(t)}^2 \\
    &= N(1-\av{\syn^1_\text{ideal}\syn^1(t)}^2),
\end{aligned}
\end{equation}
%
where we used $\syn^\alpha=\pm1$.
We can compute $\var(\syn_\text{ideal}\cdt\syn(\infty))$ by taking $t\to\infty$.

For the numerator, we can write
%
\begin{equation}\label{eq:overlapex}
  \av{\syn_\text{ideal}\cdt\syn(t)} = \sum_\alpha\av{\syn^\alpha_\text{ideal}\syn^\alpha(t)}
   = N \av{\syn^1_\text{ideal}\syn^1(t)},
\end{equation}
%
If the elements of $\synid$ take values $\pm1$ with probability $f\potdep$,
%
\begin{equation}\label{eq:overlapone}
  \av{\syn^1_\text{ideal}\syn^1(t)} = f\pot \av{\syn^1(t)}_{\text{pot},t=0} - f\dep \av{\syn^1(t)}_{\text{dep},t=0} 
\end{equation}
%
To compute this quantity, we will need to discuss how we model individual synapses.




\subsection{Markov models of synapses}\label{sec:markovsynapse}

We model a synapse as having $M$ internal states.
The synaptic weight depends deterministically on the state, given by the $M$-element column vector $\w$.
We denote the probability distribution across these states by the row vector $\pr(t)$.
We denote a column vector of ones by $\onev$, so the normalisation condition is $\pr(t)\onev=1$.

When the synapse is subjected to a plasticity event the internal state will change stochastically, described by the matrices $\M\potdep$ whose $i,j$ elements are the transition probabilities from state $i$ to state $j$.
In between these plasticity events the synapse will undergo stochastic transitions described by a matrix $\hom$ whose $i,j$ element is the transition rate from state $i$ to state $j$ for $i \neq j$ and $\hom_{ii}=-\sum_{j \neq i} \hom_{ij}$.
This allows us to describe homeostatic plasticity as well as plasticity processes that occur over a period of time -- $\M\potdep$ puts the synapse into some state where the process $\hom$ takes over and does the rest.
The matrices $\M\potdep$ are transition matrices of discrete time Markov processes, whereas $\hom$ is the transition matrix of a continuous time Markov process.
Their elements must satisfy the following constraints
%
\begin{equation}\label{eq:constr}
\begin{aligned}
  \M\potdep_{ij} &\geq 0, &\qquad
  \sum_j \M\potdep_{ij} &= 1, \\
  \hom_{i \neq j} &\geq 0, &\qquad
  \sum_j \hom_{ij} &= 0. 
\end{aligned}
\end{equation}
%
The upper bounds $\M\potdep_{ij} \leq 1$ and $\hom_{ii} \leq 0$ follow automatically from these.
There is no upper bound on $\hom_{i \neq j}$ (or lower bound on $\hom_{ii}$) as these are rates rather than probabilities.


Given a sequence of plasticity events at times $t_1,t_2,t_3,\ldots$, the probability distribution across the internal states is
%
\begin{equation}\label{eq:plastseq}
  \pr(t_n) = \pr(t_0) \e^{\hom(t_1-t_0)} \M\pot \e^{\hom(t_2-t_1)} \M\dep \e^{\hom(t_3-t_4)} \M\dep \ldots \M\pot \e^{\hom(t_n-t_{n-1})}.
\end{equation}
%
In computing the expectation in \eqref{eq:overlapone} the plasticity event that occurs at $t=0$ is fixed, but all of the previous and subsequent events are marginalised over.
If the plasticity events occur at Poisson rate $r$, with potentiation and depression occurring independently with probability $f\potdep$, averaging over all previous and subsequent events gives
%
\begin{equation}\label{eq:plastseqav}
  P(\text{state}=i,t \mid \text{pot/dep},0) = \brk{ \eq \M\potdep\, \e^{r(f\pot\M\pot+f\dep\M\dep-\I)t +\hom t} }_i,
\end{equation}
%
where $\eq$ is the steady state distribution of the continuous time transition matrix that appears in the exponential:
%
\begin{equation}\label{eq:eq}
  \eq(r\frg +\hom) = 0,
  \qquad\text{where}\quad
  \frg = f\pot\M\pot + f\dep\M\dep - \I.
\end{equation}
%



This results in
%
\begin{equation}\label{eq:overlapwt}
\begin{aligned}
  \av{\syn^1_\text{ideal}\syn^1(t)} &= \eq (f\pot\M\pot-f\dep\M\dep)\, \e^{(r\frg+\hom)t} \w, \\
  \av{\syn^1_\text{ideal}\syn^1(\infty)} &= \eq (f\pot\M\pot-f\dep\M\dep)\, \onev\eq \w \\
         &= \eq (f\pot\onev-f\dep\onev)\, \eq \w \\
         &=  (f\pot-f\dep)\, \eq \w \\
         &=  (f\pot-f\dep)\, \eq \e^{(r\frg+\hom)t} \w .
\end{aligned}
\end{equation}
%
Combining these allows us to write the numerator of \eqref{eq:SNRdef} as
%
\begin{equation}\label{eq:signal}
\begin{aligned}
  \av{\syn_\text{ideal}\cdt\syn(t)} - \av{\syn_\text{ideal}\cdt\syn(\infty)}
    &= N \eq \enc \, \e^{(r\frg+\hom)t} \w , \\
    \text{where} \qquad
  \enc &= f\pot(\M\pot-\I)-f\dep(\M\dep-\I)
\end{aligned}
\end{equation}
%
Combining with \eqref{eq:noise} gives
%
\begin{equation}\label{eq:SNRcurveExact}
  \SNR(t) = \frac{\sqrt{N} \eq \enc\,\e^{(r\frg+\hom)t}\w}
                 {\sqrt{1-(f\pot-f\dep)^2(\eq_+-\eq_-)^2}}.
\end{equation}
%
We can absorb $\hom$ into $\MM\potdep$ by defining
%
\begin{equation}\label{eq:absorbhom}
\begin{aligned}
  \Mh\potdep &= \M\potdep + \frac{\hom}{2rf\potdep},
  &\qquad
  \frgh &= f\pot\Mh\pot + f\dep\Mh\dep - \I,
  \\&&
  \ench &= f\pot(\Mh\pot-\I)-f\dep(\Mh\dep-\I).
\end{aligned}
\end{equation}
%
Then the SNR curve is
%
\begin{equation}\label{eq:SNRcurvehom}
  \SNR(t) = \frac{\sqrt{N}  \eq \ench \,\e^{rt\frgh}\w}
                 {\sqrt{1-(f\pot-f\dep)^2(\eq_+-\eq_-)^2}}.
\end{equation}
%
The denominator will not play any role in what follows, as the models that maximize the various measures of memory performance all have some sort of balance between potentiation and depression, either with $f\pot=f\dep$ or $\eq_+=\eq_-$.
We can perform the maximisation in two steps.
First maximise the numerator at fixed $\eq\w$.
Then maximise the ratio \wrt $\eq\w$.
This will allow us to ignore the denominator until the very end.



The constraints on $\Mh\potdep$ are less stringent than the constraints on $\M\potdep$, \eqref{eq:constr}.
Clearly
%
\begin{equation}\label{eq:constrhoffdiag}
  \Mh\potdep_{i \neq j} \geq 0, \qquad
  \sum_j \Mh\potdep_{ij} = 1.
\end{equation}
%
The ``off-diagonal'' constraints are more complicated.
There can be many $\M\potdep,\hom$ that result in the same $\Mh\potdep$.
However, the fact that $\M\potdep_{ij} \in [0,1]$ and $\hom_{i\neq j} \geq 0$ imply that $\forall (i \neq j)$:
%
\begin{equation}\label{eq:homrange}
  2r\max\brc{f\pot(\Mh\pot_{ij}-1),f\dep(\Mh\dep_{ij}-1)} \leq \hom_{ij} \leq 2r\min\brc{f\pot\Mh\pot_{ij},f\dep\Mh\dep_{ij}}
\end{equation}
%
These two bounds are consistent when
%
\begin{equation}\label{eq:homconsistent}
  -f\dep \leq f\pot\Mh\pot_{ij} - f\dep\Mh\dep_{ij} \leq f\pot
  \qquad \forall i\neq j.
\end{equation}
%
To ensure that $\M\potdep_{ii} \geq 0$, we also need to ensure that $\sum_{j\neq i}\M\potdep_{ij} \leq 1$.
%
\begin{equation}\label{eq:homdiag}
\begin{aligned}
  \sum_{j\neq i} \M\pot_{ij} 
    &= \sum_{j\neq i} \prn{ \Mh\pot_{ij} - \frac{\hom_{ij}}{2rf\pot} }\\
    &\geq \sum_{j\neq i} \prn{ \Mh\pot_{ij} - \frac{1}{f\pot}  \min\brc{ f\pot\Mh\pot_{ij},f\dep\Mh\dep_{ij} } }\\
    &= \frac{1}{f\pot} \sum_{j\neq i} \brk{ f\pot\Mh\pot_{ij}-f\dep\Mh\dep_{ij} }_+ , \\
  \sum _{j\neq i} \M\dep_{ij} 
    &= \frac{1}{f\dep} \sum_{j\neq i} \brk{ f\dep\Mh\dep_{ij}-f\pot\Mh\pot_{ij} }_+ ,
\end{aligned}
\end{equation}
%
where rectification function is $[x]_\pm = x \Theta(\pm x)$, in terms of the Heaviside step function.
Demanding that these quantities are at most 1 is sufficient to ensure that \eqref{eq:homconsistent} is satisfied.

Therefore we find the set of constraints
%
\begin{equation}\label{eq:constrh}
\begin{aligned}
  \Mh\potdep_{i \neq j} &\geq 0, &\qquad
  \sum_{j\neq i} \brk{ f\pot\Mh\pot_{ij}-f\dep\Mh\dep_{ij} }_+ &\leq f\pot, \\
  \sum_j \Mh\potdep_{ij} &=1, &
  \sum_{j\neq i} \brk{ f\dep\Mh\dep_{ij}-f\pot\Mh\pot_{ij} }_+  &\leq f\dep.
\end{aligned}
\end{equation}
%
We showed that these are necessary conditions for the existence of $\M\potdep,\hom$ satisfying \eqref{eq:constr}.
We can see that they are sufficient by choosing $\hom_{ij} = 2r\min\brc{f\pot\Mh\pot_{ij},f\dep\Mh\dep_{ij}}$ for $i \neq j$ and $\hom_{ii}=-\sum_{j \neq i} \hom_{ij}$.

We can then treat the off-diagonal elements of $\Mh\potdep$ as the independent degrees of freedom, subject to these constraints, when maximising the SNR.


%\subsection*{Acknowledgements}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection*{Appendices}
%\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{utcaps_sl}
\bibliography{maths,neuro,markov}

\end{document}
